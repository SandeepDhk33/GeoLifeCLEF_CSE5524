{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ca0884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import rasterio\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "from transformers import SwinModel\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "863bd09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_patch_path(data_path, survey_id):\n",
    "    \"\"\"Construct the patch file path based on plot_id as './CD/AB/XXXXABCD.tiff'\"\"\"\n",
    "    path = data_path\n",
    "    for d in (str(survey_id)[-2:], str(survey_id)[-4:-2]):\n",
    "        path = os.path.join(path, d)\n",
    "\n",
    "    path = os.path.join(path, f\"{survey_id}.tiff\")\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b16b9900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_normalize(band, low=2, high=98):\n",
    "    sorted_band = np.sort(band.flatten())\n",
    "    quantiles = np.percentile(sorted_band, np.linspace(low, high, len(sorted_band)))\n",
    "    normalized_band = np.interp(band.flatten(), sorted_band, quantiles).reshape(band.shape)\n",
    "    \n",
    "    min_val, max_val = np.min(normalized_band), np.max(normalized_band)\n",
    "    \n",
    "    # Prevent division by zero if min_val == max_val\n",
    "    if max_val == min_val:\n",
    "        return np.zeros_like(normalized_band, dtype=np.float32)  # Return an array of zeros\n",
    "\n",
    "    # Perform normalization (min-max scaling)\n",
    "    return ((normalized_band - min_val) / (max_val - min_val)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d22be05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, bioclim_data_dir, landsat_data_dir, sentinel_data_dir, metadata, transform=None):\n",
    "        self.transform = transform\n",
    "        self.sentinel_transform = transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.5, 0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "      \n",
    "        self.bioclim_data_dir = bioclim_data_dir\n",
    "        self.landsat_data_dir = landsat_data_dir\n",
    "        self.sentinel_data_dir = sentinel_data_dir\n",
    "        self.metadata = metadata\n",
    "        self.metadata = self.metadata.dropna(subset=\"speciesId\").reset_index(drop=True)\n",
    "        self.metadata['speciesId'] = self.metadata['speciesId'].astype(int)\n",
    "        self.label_dict = self.metadata.groupby('surveyId')['speciesId'].apply(list).to_dict()\n",
    "        \n",
    "        self.metadata = self.metadata.drop_duplicates(subset=\"surveyId\").reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        survey_id = self.metadata.surveyId[idx]\n",
    "        landsat_sample = torch.nan_to_num(torch.load(os.path.join(self.landsat_data_dir, f\"GLC25-PA-train-landsat-time-series_{survey_id}_cube.pt\"), weights_only=True))\n",
    "        bioclim_sample = torch.load(os.path.join(self.bioclim_data_dir, f\"GLC25-PA-train-bioclimatic_monthly_{survey_id}_cube.pt\"), weights_only=True)\n",
    "        \n",
    "        # Read TIFF files (multispectral bands)\n",
    "        tiff_path = construct_patch_path(self.sentinel_data_dir, survey_id)\n",
    "        with rasterio.open(tiff_path) as dataset:\n",
    "            sentinel_sample = dataset.read(out_dtype=np.float32)  # Read all bands\n",
    "            sentinel_sample = np.array([quantile_normalize(band) for band in sentinel_sample])  # Apply quantile normalization\n",
    "\n",
    "        sentinel_sample = np.transpose(sentinel_sample, (1, 2, 0))  # Convert to HWC format\n",
    "        \n",
    "        species_ids = self.label_dict.get(survey_id, [])  # Get list of species IDs for the survey ID\n",
    "        label = torch.zeros(num_classes)  # Initialize label tensor\n",
    "        for species_id in species_ids:\n",
    "            label_id = species_id\n",
    "            label[label_id] = 1  # Set the corresponding class index to 1 for each species\n",
    "        \n",
    "        if isinstance(landsat_sample, torch.Tensor):\n",
    "            landsat_sample = landsat_sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            landsat_sample = landsat_sample.numpy()  # Convert tensor to numpy array\n",
    "            \n",
    "        if isinstance(bioclim_sample, torch.Tensor):\n",
    "            bioclim_sample = bioclim_sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            bioclim_sample = bioclim_sample.numpy()  # Convert tensor to numpy array \n",
    "        \n",
    "        if self.transform:\n",
    "            landsat_sample = self.transform(landsat_sample)\n",
    "            bioclim_sample = self.transform(bioclim_sample)\n",
    "            sentinel_sample = self.sentinel_transform(sentinel_sample)\n",
    "\n",
    "        return landsat_sample, bioclim_sample, sentinel_sample, label, survey_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adde3fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, bioclim_data_dir, landsat_data_dir, sentinel_data_dir, metadata, transform=None):\n",
    "        self.transform = transform\n",
    "        self.sentinel_transform = transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.5, 0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "      \n",
    "        self.bioclim_data_dir = bioclim_data_dir\n",
    "        self.landsat_data_dir = landsat_data_dir\n",
    "        self.sentinel_data_dir = sentinel_data_dir\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        survey_id = self.metadata.surveyId[idx]\n",
    "        landsat_sample = torch.nan_to_num(torch.load(os.path.join(self.landsat_data_dir, f\"GLC25-PA-train-landsat-time-series_{survey_id}_cube.pt\"), weights_only=True))\n",
    "        bioclim_sample = torch.load(os.path.join(self.bioclim_data_dir, f\"GLC25-PA-train-bioclimatic_monthly_{survey_id}_cube.pt\"), weights_only=True)\n",
    "        \n",
    "        # Read TIFF files (multispectral bands)\n",
    "        tiff_path = construct_patch_path(self.sentinel_data_dir, survey_id)\n",
    "        with rasterio.open(tiff_path) as dataset:\n",
    "            sentinel_sample = dataset.read(out_dtype=np.float32)  # Read all bands\n",
    "            sentinel_sample = np.array([quantile_normalize(band) for band in sentinel_sample])  # Apply quantile normalization\n",
    "\n",
    "        sentinel_sample = np.transpose(sentinel_sample, (1, 2, 0))  # Convert to HWC format\n",
    "\n",
    "        if isinstance(landsat_sample, torch.Tensor):\n",
    "            landsat_sample = landsat_sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            landsat_sample = landsat_sample.numpy()  # Convert tensor to numpy array\n",
    "            \n",
    "        if isinstance(bioclim_sample, torch.Tensor):\n",
    "            bioclim_sample = bioclim_sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            bioclim_sample = bioclim_sample.numpy()  # Convert tensor to numpy array   \n",
    "        \n",
    "        if self.transform:\n",
    "            landsat_sample = self.transform(landsat_sample)\n",
    "            bioclim_sample = self.transform(bioclim_sample)\n",
    "            sentinel_sample = self.sentinel_transform(sentinel_sample)\n",
    "\n",
    "        return landsat_sample, bioclim_sample, sentinel_sample, survey_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d59e8702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create small dataset for initial model development\n",
    "# train_metadata_path = \"/fs/scratch/PAS2985/group_23/training_data.csv\"\n",
    "# val_metadata_path = \"/fs/scratch/PAS2985/group_23/val_data.csv\"\n",
    "# test_metadata_path = \"/fs/scratch/PAS2985/group_23/test_data.csv\"\n",
    "\n",
    "# train_metadata = pd.read_csv(train_metadata_path)\n",
    "# val_metadata = pd.read_csv(val_metadata_path)\n",
    "# test_metadata = pd.read_csv(test_metadata_path)\n",
    "\n",
    "# # Sample 10% of each\n",
    "# train_sample = train_metadata.sample(frac=0.1, random_state=42)\n",
    "# val_sample = val_metadata.sample(frac=0.1, random_state=42)\n",
    "# test_sample = test_metadata.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# # Save to new CSVs\n",
    "# train_sample.to_csv(\"/users/PAS2956/sandeep633/training_data_10.csv\", index=False)\n",
    "# val_sample.to_csv(\"/users/PAS2956/sandeep633/val_data_10.csv\", index=False)\n",
    "# test_sample.to_csv(\"/users/PAS2956/sandeep633/test_data_10.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "874ceb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and DataLoader\n",
    "batch_size = 32\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load Training metadata\n",
    "bioclim_data_path = \"/fs/scratch/PAS2985/group_23/BioclimTimeSeries/cubes/PA-train/\"\n",
    "landsat_data_path = \"/fs/scratch/PAS2985/group_23/SateliteTimeSeries-Landsat/cubes/PA-train/\"\n",
    "sentinel_data_path = \"/fs/scratch/PAS2985/group_23/SatelitePatches/PA-train/\"\n",
    "\n",
    "## 10% of the entire dataset\n",
    "# /users/PAS2956/sandeep633/training_data_10.csv\n",
    "# /users/PAS2956/sandeep633/val_data_10.csv\n",
    "# /users/PAS2956/sandeep633/test_data_10.csv\n",
    "\n",
    "## Complete Dataset\n",
    "# /fs/scratch/PAS2985/group_23/training_data.csv\n",
    "# /fs/scratch/PAS2985/group_23/val_data.csv\n",
    "# /fs/scratch/PAS2985/group_23/test_data.csv\n",
    "\n",
    "train_metadata_path = \"/fs/scratch/PAS2985/group_23/training_data.csv\"\n",
    "val_metadata_path = \"/fs/scratch/PAS2985/group_23/val_data.csv\"\n",
    "test_metadata_path = \"/fs/scratch/PAS2985/group_23/test_data.csv\"\n",
    "\n",
    "train_metadata = pd.read_csv(train_metadata_path)\n",
    "train_dataset = TrainDataset(bioclim_data_path, landsat_data_path, sentinel_data_path, train_metadata, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "val_metadata = pd.read_csv(val_metadata_path)\n",
    "val_dataset = TrainDataset(bioclim_data_path, landsat_data_path, sentinel_data_path, val_metadata, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "test_metadata = pd.read_csv(test_metadata_path)\n",
    "test_dataset = TestDataset(bioclim_data_path, landsat_data_path, sentinel_data_path, test_metadata, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3bfb835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1037995\n",
      "222611\n",
      "223031\n"
     ]
    }
   ],
   "source": [
    "print(len(train_metadata))\n",
    "print(len(val_metadata))\n",
    "print(len(test_metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37dc4c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1947\n",
      "418\n",
      "6970\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "print(len(val_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f84f6e",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a35322b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(predicted, test_metadata, top_k=25):\n",
    "    # Convert the predictions string to a list of integers\n",
    "    predicted['predictions'] = predicted['predictions'].apply(lambda predicted: list(map(int, predicted.split())))\n",
    "    \n",
    "    # Create a dictionary of {surveyId: predicted_species_list}\n",
    "    pred_dict = dict(zip(predicted['surveyId'], predicted['predictions']))\n",
    "    \n",
    "    # Prepare ground truth - group by surveyId to get all true species for each survey\n",
    "    true_dict = test_metadata.groupby('surveyId')['speciesId'].apply(list).to_dict()\n",
    "    \n",
    "    # Binary accuracy (is true species in top-25?)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for survey_id, true_species_list in true_dict.items():\n",
    "        if survey_id in pred_dict:\n",
    "            pred_species = pred_dict[survey_id]\n",
    "            # Check if any of the true species is in the top-25 predictions\n",
    "            for species in true_species_list:\n",
    "                total += 1\n",
    "                if species in pred_species:\n",
    "                    correct += 1\n",
    "    \n",
    "    binary_accuracy = correct / total\n",
    "    print(f\"Binary Accuracy (is true species in top-25?): {binary_accuracy:.4f}\")\n",
    "    \n",
    "    # F1-score (micro-averaged)\n",
    "    # For this we need to create binary vectors for predictions and ground truth\n",
    "    all_species = set()\n",
    "    for species_list in true_dict.values():\n",
    "        all_species.update(species_list)\n",
    "    for species_list in pred_dict.values():\n",
    "        all_species.update(species_list)\n",
    "    \n",
    "    all_species = sorted(all_species)\n",
    "    species_to_idx = {species: idx for idx, species in enumerate(all_species)}\n",
    "    \n",
    "    # Create binary vectors\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for survey_id, true_species_list in true_dict.items():\n",
    "        if survey_id in pred_dict:\n",
    "            # Ground truth vector\n",
    "            true_vec = [0] * len(all_species)\n",
    "            for species in true_species_list:\n",
    "                true_vec[species_to_idx[species]] = 1\n",
    "            \n",
    "            # Prediction vector (top-25 are marked as 1)\n",
    "            pred_vec = [0] * len(all_species)\n",
    "            for species in pred_dict[survey_id]:\n",
    "                if species in species_to_idx:  # In case some predicted species aren't in ground truth\n",
    "                    pred_vec[species_to_idx[species]] = 1\n",
    "            \n",
    "            y_true.append(true_vec)\n",
    "            y_pred.append(pred_vec)\n",
    "    \n",
    "    # Calculate micro F1-score (considers all species equally)\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "    print(f\"Micro F1-score: {micro_f1:.4f}\")\n",
    "    \n",
    "    # Calculate macro F1-score (average per species)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    print(f\"Macro F1-score: {macro_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8102464",
   "metadata": {},
   "source": [
    "## Advanced model-2 (Resnet 18, Resnet 18, Swin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dccb9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalEnsemble_Baseline(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultimodalEnsemble_Baseline, self).__init__()\n",
    "        \n",
    "        self.landsat_norm = nn.LayerNorm([6,4,21])\n",
    "        self.landsat_model = models.resnet18(weights=None)\n",
    "        # Modify the first convolutional layer to accept 6 channels instead of 3\n",
    "        self.landsat_model.conv1 = nn.Conv2d(6, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.landsat_model.maxpool = nn.Identity()\n",
    "        \n",
    "        self.bioclim_norm = nn.LayerNorm([4,19,12])\n",
    "        self.bioclim_model = models.resnet18(weights=None)  \n",
    "        # Modify the first convolutional layer to accept 4 channels instead of 3\n",
    "        self.bioclim_model.conv1 = nn.Conv2d(4, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bioclim_model.maxpool = nn.Identity()\n",
    "        \n",
    "        self.sentinel_model = models.swin_t(weights=\"IMAGENET1K_V1\")\n",
    "        # Modify the first layer to accept 4 channels instead of 3\n",
    "        self.sentinel_model.features[0][0] = nn.Conv2d(4, 96, kernel_size=(4, 4), stride=(4, 4))\n",
    "        self.sentinel_model.head = nn.Identity()\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(1000)\n",
    "        self.ln2 = nn.LayerNorm(1000)\n",
    "        self.fc1 = nn.Linear(2768, 4096)\n",
    "        self.fc2 = nn.Linear(4096, num_classes)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "    def forward(self, x, y, z):\n",
    "        \n",
    "        x = self.landsat_norm(x)\n",
    "        x = self.landsat_model(x)\n",
    "        x = self.ln1(x)\n",
    "        \n",
    "        y = self.bioclim_norm(y)\n",
    "        y = self.bioclim_model(y)\n",
    "        y = self.ln2(y)\n",
    "        \n",
    "        z = self.sentinel_model(z)\n",
    "        \n",
    "        xyz = torch.cat((x, y, z), dim=1)\n",
    "        xyz = self.fc1(xyz)\n",
    "        xyz = self.dropout(xyz)\n",
    "        out = self.fc2(xyz)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "773a7af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    # Set seed for Python's built-in random number generator\n",
    "    torch.manual_seed(seed)\n",
    "    # Set seed for numpy\n",
    "    np.random.seed(seed)\n",
    "    # Set seed for CUDA if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # Set cuDNN's random number generator seed for deterministic behavior\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c90b191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = CUDA\n"
     ]
    }
   ],
   "source": [
    "# Check if cuda is available\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"DEVICE = CUDA\")\n",
    "    \n",
    "num_classes = 11255\n",
    "model = MultimodalEnsemble_Baseline(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b653463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.00025\n",
    "num_epochs = 10\n",
    "positive_weigh_factor = 1.0\n",
    "num_classes = 11255\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0aeeaef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10 epochs started.\n",
      "Epoch 1/10, Batch 0/1947, Loss: 0.7039719820022583\n",
      "Epoch 1/10, Batch 50/1947, Loss: 0.007637599483132362\n",
      "Epoch 1/10, Batch 100/1947, Loss: 0.005976442247629166\n",
      "Epoch 1/10, Batch 150/1947, Loss: 0.0072599309496581554\n",
      "Epoch 1/10, Batch 200/1947, Loss: 0.005846166051924229\n",
      "Epoch 1/10, Batch 250/1947, Loss: 0.005352099891752005\n",
      "Epoch 1/10, Batch 300/1947, Loss: 0.006282826419919729\n",
      "Epoch 1/10, Batch 350/1947, Loss: 0.005680120084434748\n",
      "Epoch 1/10, Batch 400/1947, Loss: 0.005495390854775906\n",
      "Epoch 1/10, Batch 450/1947, Loss: 0.0070854597724974155\n",
      "Epoch 1/10, Batch 500/1947, Loss: 0.0050410437397658825\n",
      "Epoch 1/10, Batch 550/1947, Loss: 0.005127918440848589\n",
      "Epoch 1/10, Batch 600/1947, Loss: 0.005816277582198381\n",
      "Epoch 1/10, Batch 650/1947, Loss: 0.006468119565397501\n",
      "Epoch 1/10, Batch 700/1947, Loss: 0.0055840350687503815\n",
      "Epoch 1/10, Batch 750/1947, Loss: 0.005047069862484932\n",
      "Epoch 1/10, Batch 800/1947, Loss: 0.005462569184601307\n",
      "Epoch 1/10, Batch 850/1947, Loss: 0.004768643528223038\n",
      "Epoch 1/10, Batch 900/1947, Loss: 0.005244140047580004\n",
      "Epoch 1/10, Batch 950/1947, Loss: 0.005010815802961588\n",
      "Epoch 1/10, Batch 1000/1947, Loss: 0.004948923829942942\n",
      "Epoch 1/10, Batch 1050/1947, Loss: 0.0047632670029997826\n",
      "Epoch 1/10, Batch 1100/1947, Loss: 0.005026772152632475\n",
      "Epoch 1/10, Batch 1150/1947, Loss: 0.0058319540694355965\n",
      "Epoch 1/10, Batch 1200/1947, Loss: 0.005256858188658953\n",
      "Epoch 1/10, Batch 1250/1947, Loss: 0.004734635353088379\n",
      "Epoch 1/10, Batch 1300/1947, Loss: 0.005334727931767702\n",
      "Epoch 1/10, Batch 1350/1947, Loss: 0.004619346931576729\n",
      "Epoch 1/10, Batch 1400/1947, Loss: 0.004572768695652485\n",
      "Epoch 1/10, Batch 1450/1947, Loss: 0.005414930637925863\n",
      "Epoch 1/10, Batch 1500/1947, Loss: 0.004600480664521456\n",
      "Epoch 1/10, Batch 1550/1947, Loss: 0.004859336651861668\n",
      "Epoch 1/10, Batch 1600/1947, Loss: 0.005350011866539717\n",
      "Epoch 1/10, Batch 1650/1947, Loss: 0.005057075060904026\n",
      "Epoch 1/10, Batch 1700/1947, Loss: 0.004430999979376793\n",
      "Epoch 1/10, Batch 1750/1947, Loss: 0.004518371541053057\n",
      "Epoch 1/10, Batch 1800/1947, Loss: 0.0050763580948114395\n",
      "Epoch 1/10, Batch 1850/1947, Loss: 0.003525568637996912\n",
      "Epoch 1/10, Batch 1900/1947, Loss: 0.004729588516056538\n",
      "New best model saved with val loss: 0.0049\n",
      "\n",
      "Epoch 1 Summary:\n",
      "Train Loss: 0.0062 | Val Loss: 0.0049\n",
      "Epoch 2/10, Batch 0/1947, Loss: 0.004496503621339798\n",
      "Epoch 2/10, Batch 50/1947, Loss: 0.005883011035621166\n",
      "Epoch 2/10, Batch 100/1947, Loss: 0.004326744005084038\n",
      "Epoch 2/10, Batch 150/1947, Loss: 0.004259152803570032\n",
      "Epoch 2/10, Batch 200/1947, Loss: 0.004695422947406769\n",
      "Epoch 2/10, Batch 250/1947, Loss: 0.005373965948820114\n",
      "Epoch 2/10, Batch 300/1947, Loss: 0.00442964443936944\n",
      "Epoch 2/10, Batch 350/1947, Loss: 0.0052603683434426785\n",
      "Epoch 2/10, Batch 400/1947, Loss: 0.00424801092594862\n",
      "Epoch 2/10, Batch 450/1947, Loss: 0.004455674905329943\n",
      "Epoch 2/10, Batch 500/1947, Loss: 0.004690191242843866\n",
      "Epoch 2/10, Batch 550/1947, Loss: 0.004221615847200155\n",
      "Epoch 2/10, Batch 600/1947, Loss: 0.004874633625149727\n",
      "Epoch 2/10, Batch 650/1947, Loss: 0.0045332107692956924\n",
      "Epoch 2/10, Batch 700/1947, Loss: 0.005179277155548334\n",
      "Epoch 2/10, Batch 750/1947, Loss: 0.0042817299254238605\n",
      "Epoch 2/10, Batch 800/1947, Loss: 0.004465307109057903\n",
      "Epoch 2/10, Batch 850/1947, Loss: 0.0041297185234725475\n",
      "Epoch 2/10, Batch 900/1947, Loss: 0.0045449682511389256\n",
      "Epoch 2/10, Batch 950/1947, Loss: 0.004428391344845295\n",
      "Epoch 2/10, Batch 1000/1947, Loss: 0.0040391553193330765\n",
      "Epoch 2/10, Batch 1050/1947, Loss: 0.004491686820983887\n",
      "Epoch 2/10, Batch 1100/1947, Loss: 0.004684354178607464\n",
      "Epoch 2/10, Batch 1150/1947, Loss: 0.004714324604719877\n",
      "Epoch 2/10, Batch 1200/1947, Loss: 0.004445267375558615\n",
      "Epoch 2/10, Batch 1250/1947, Loss: 0.004974668379873037\n",
      "Epoch 2/10, Batch 1300/1947, Loss: 0.004164010286331177\n",
      "Epoch 2/10, Batch 1350/1947, Loss: 0.004610312636941671\n",
      "Epoch 2/10, Batch 1400/1947, Loss: 0.004360951483249664\n",
      "Epoch 2/10, Batch 1450/1947, Loss: 0.0041145808063447475\n",
      "Epoch 2/10, Batch 1500/1947, Loss: 0.00500003807246685\n",
      "Epoch 2/10, Batch 1550/1947, Loss: 0.004946554079651833\n",
      "Epoch 2/10, Batch 1600/1947, Loss: 0.003962208516895771\n",
      "Epoch 2/10, Batch 1650/1947, Loss: 0.0039471169002354145\n",
      "Epoch 2/10, Batch 1700/1947, Loss: 0.003994823433458805\n",
      "Epoch 2/10, Batch 1750/1947, Loss: 0.004832826089113951\n",
      "Epoch 2/10, Batch 1800/1947, Loss: 0.00387388002127409\n",
      "Epoch 2/10, Batch 1850/1947, Loss: 0.0036709855776280165\n",
      "Epoch 2/10, Batch 1900/1947, Loss: 0.004688311368227005\n",
      "New best model saved with val loss: 0.0045\n",
      "\n",
      "Epoch 2 Summary:\n",
      "Train Loss: 0.0047 | Val Loss: 0.0045\n",
      "Epoch 3/10, Batch 0/1947, Loss: 0.004849917721003294\n",
      "Epoch 3/10, Batch 50/1947, Loss: 0.0046635218895971775\n",
      "Epoch 3/10, Batch 100/1947, Loss: 0.003759851912036538\n",
      "Epoch 3/10, Batch 150/1947, Loss: 0.0043531861156225204\n",
      "Epoch 3/10, Batch 200/1947, Loss: 0.00425336230546236\n",
      "Epoch 3/10, Batch 250/1947, Loss: 0.004391730297356844\n",
      "Epoch 3/10, Batch 300/1947, Loss: 0.004864895716309547\n",
      "Epoch 3/10, Batch 350/1947, Loss: 0.004580209497362375\n",
      "Epoch 3/10, Batch 400/1947, Loss: 0.004669884219765663\n",
      "Epoch 3/10, Batch 450/1947, Loss: 0.0044376300647854805\n",
      "Epoch 3/10, Batch 500/1947, Loss: 0.004394914023578167\n",
      "Epoch 3/10, Batch 550/1947, Loss: 0.00453987019136548\n",
      "Epoch 3/10, Batch 600/1947, Loss: 0.0034820977598428726\n",
      "Epoch 3/10, Batch 650/1947, Loss: 0.0038111174944788218\n",
      "Epoch 3/10, Batch 700/1947, Loss: 0.0041894568130373955\n",
      "Epoch 3/10, Batch 750/1947, Loss: 0.004567145835608244\n",
      "Epoch 3/10, Batch 800/1947, Loss: 0.005434331018477678\n",
      "Epoch 3/10, Batch 850/1947, Loss: 0.005260963458567858\n",
      "Epoch 3/10, Batch 900/1947, Loss: 0.0046438914723694324\n",
      "Epoch 3/10, Batch 950/1947, Loss: 0.004103173967450857\n",
      "Epoch 3/10, Batch 1000/1947, Loss: 0.0038229795172810555\n",
      "Epoch 3/10, Batch 1050/1947, Loss: 0.003640951821580529\n",
      "Epoch 3/10, Batch 1100/1947, Loss: 0.004830758552998304\n",
      "Epoch 3/10, Batch 1150/1947, Loss: 0.00545938964933157\n",
      "Epoch 3/10, Batch 1200/1947, Loss: 0.005278189666569233\n",
      "Epoch 3/10, Batch 1250/1947, Loss: 0.005052100867033005\n",
      "Epoch 3/10, Batch 1300/1947, Loss: 0.004048772621899843\n",
      "Epoch 3/10, Batch 1350/1947, Loss: 0.004664387088268995\n",
      "Epoch 3/10, Batch 1400/1947, Loss: 0.004215070977807045\n",
      "Epoch 3/10, Batch 1450/1947, Loss: 0.004190774634480476\n",
      "Epoch 3/10, Batch 1500/1947, Loss: 0.004652706440538168\n",
      "Epoch 3/10, Batch 1550/1947, Loss: 0.003675063606351614\n",
      "Epoch 3/10, Batch 1600/1947, Loss: 0.004166780039668083\n",
      "Epoch 3/10, Batch 1650/1947, Loss: 0.004300176165997982\n",
      "Epoch 3/10, Batch 1700/1947, Loss: 0.003960099536925554\n",
      "Epoch 3/10, Batch 1750/1947, Loss: 0.004243710543960333\n",
      "Epoch 3/10, Batch 1800/1947, Loss: 0.004997610114514828\n",
      "Epoch 3/10, Batch 1850/1947, Loss: 0.004394071642309427\n",
      "Epoch 3/10, Batch 1900/1947, Loss: 0.004037876147776842\n",
      "New best model saved with val loss: 0.0044\n",
      "\n",
      "Epoch 3 Summary:\n",
      "Train Loss: 0.0045 | Val Loss: 0.0044\n",
      "Epoch 4/10, Batch 0/1947, Loss: 0.004141767509281635\n",
      "Epoch 4/10, Batch 50/1947, Loss: 0.005248597357422113\n",
      "Epoch 4/10, Batch 100/1947, Loss: 0.004134542308747768\n",
      "Epoch 4/10, Batch 150/1947, Loss: 0.004139742348343134\n",
      "Epoch 4/10, Batch 200/1947, Loss: 0.004586625844240189\n",
      "Epoch 4/10, Batch 250/1947, Loss: 0.004152508452534676\n",
      "Epoch 4/10, Batch 300/1947, Loss: 0.005137484520673752\n",
      "Epoch 4/10, Batch 350/1947, Loss: 0.004104530904442072\n",
      "Epoch 4/10, Batch 400/1947, Loss: 0.004208588972687721\n",
      "Epoch 4/10, Batch 450/1947, Loss: 0.005007344763725996\n",
      "Epoch 4/10, Batch 500/1947, Loss: 0.005277994554489851\n",
      "Epoch 4/10, Batch 550/1947, Loss: 0.004240420181304216\n",
      "Epoch 4/10, Batch 600/1947, Loss: 0.004278057254850864\n",
      "Epoch 4/10, Batch 650/1947, Loss: 0.0034650429151952267\n",
      "Epoch 4/10, Batch 700/1947, Loss: 0.004535934887826443\n",
      "Epoch 4/10, Batch 750/1947, Loss: 0.004073713906109333\n",
      "Epoch 4/10, Batch 800/1947, Loss: 0.004099552053958178\n",
      "Epoch 4/10, Batch 850/1947, Loss: 0.0044274707324802876\n",
      "Epoch 4/10, Batch 900/1947, Loss: 0.0052950126118958\n",
      "Epoch 4/10, Batch 950/1947, Loss: 0.004207731690257788\n",
      "Epoch 4/10, Batch 1000/1947, Loss: 0.004129351116716862\n",
      "Epoch 4/10, Batch 1050/1947, Loss: 0.004859798587858677\n",
      "Epoch 4/10, Batch 1100/1947, Loss: 0.004706521984189749\n",
      "Epoch 4/10, Batch 1150/1947, Loss: 0.0039000585675239563\n",
      "Epoch 4/10, Batch 1200/1947, Loss: 0.00412736414000392\n",
      "Epoch 4/10, Batch 1250/1947, Loss: 0.0043191490694880486\n",
      "Epoch 4/10, Batch 1300/1947, Loss: 0.0044014910236001015\n",
      "Epoch 4/10, Batch 1350/1947, Loss: 0.004201096016913652\n",
      "Epoch 4/10, Batch 1400/1947, Loss: 0.003995969891548157\n",
      "Epoch 4/10, Batch 1450/1947, Loss: 0.004349534399807453\n",
      "Epoch 4/10, Batch 1500/1947, Loss: 0.003946153912693262\n",
      "Epoch 4/10, Batch 1550/1947, Loss: 0.004405070096254349\n",
      "Epoch 4/10, Batch 1600/1947, Loss: 0.00459627527743578\n",
      "Epoch 4/10, Batch 1650/1947, Loss: 0.003784746862947941\n",
      "Epoch 4/10, Batch 1700/1947, Loss: 0.004393475130200386\n",
      "Epoch 4/10, Batch 1750/1947, Loss: 0.004285216331481934\n",
      "Epoch 4/10, Batch 1800/1947, Loss: 0.004418341908603907\n",
      "Epoch 4/10, Batch 1850/1947, Loss: 0.00471425149589777\n",
      "Epoch 4/10, Batch 1900/1947, Loss: 0.004174873232841492\n",
      "New best model saved with val loss: 0.0043\n",
      "\n",
      "Epoch 4 Summary:\n",
      "Train Loss: 0.0043 | Val Loss: 0.0043\n",
      "Epoch 5/10, Batch 0/1947, Loss: 0.0044129351153969765\n",
      "Epoch 5/10, Batch 50/1947, Loss: 0.00405805092304945\n",
      "Epoch 5/10, Batch 100/1947, Loss: 0.004253246821463108\n",
      "Epoch 5/10, Batch 150/1947, Loss: 0.00423572538420558\n",
      "Epoch 5/10, Batch 200/1947, Loss: 0.00369440414942801\n",
      "Epoch 5/10, Batch 250/1947, Loss: 0.004629482049494982\n",
      "Epoch 5/10, Batch 300/1947, Loss: 0.004037751816213131\n",
      "Epoch 5/10, Batch 350/1947, Loss: 0.004153014160692692\n",
      "Epoch 5/10, Batch 400/1947, Loss: 0.004081894177943468\n",
      "Epoch 5/10, Batch 450/1947, Loss: 0.003921665251255035\n",
      "Epoch 5/10, Batch 500/1947, Loss: 0.004366605076938868\n",
      "Epoch 5/10, Batch 550/1947, Loss: 0.0050606695003807545\n",
      "Epoch 5/10, Batch 600/1947, Loss: 0.0037221149541437626\n",
      "Epoch 5/10, Batch 650/1947, Loss: 0.004651800729334354\n",
      "Epoch 5/10, Batch 700/1947, Loss: 0.0035056930501013994\n",
      "Epoch 5/10, Batch 750/1947, Loss: 0.003851555986329913\n",
      "Epoch 5/10, Batch 800/1947, Loss: 0.004601277876645327\n",
      "Epoch 5/10, Batch 850/1947, Loss: 0.004328118171542883\n",
      "Epoch 5/10, Batch 900/1947, Loss: 0.004505636636167765\n",
      "Epoch 5/10, Batch 950/1947, Loss: 0.0045422837138175964\n",
      "Epoch 5/10, Batch 1000/1947, Loss: 0.004445276688784361\n",
      "Epoch 5/10, Batch 1050/1947, Loss: 0.003665315918624401\n",
      "Epoch 5/10, Batch 1100/1947, Loss: 0.004016634542495012\n",
      "Epoch 5/10, Batch 1150/1947, Loss: 0.003668752498924732\n",
      "Epoch 5/10, Batch 1200/1947, Loss: 0.003843053011223674\n",
      "Epoch 5/10, Batch 1250/1947, Loss: 0.0036938919220119715\n",
      "Epoch 5/10, Batch 1300/1947, Loss: 0.004063914064317942\n",
      "Epoch 5/10, Batch 1350/1947, Loss: 0.004404217004776001\n",
      "Epoch 5/10, Batch 1400/1947, Loss: 0.004195310175418854\n",
      "Epoch 5/10, Batch 1450/1947, Loss: 0.004404337611049414\n",
      "Epoch 5/10, Batch 1500/1947, Loss: 0.004546215757727623\n",
      "Epoch 5/10, Batch 1550/1947, Loss: 0.004124303814023733\n",
      "Epoch 5/10, Batch 1600/1947, Loss: 0.004194954875856638\n",
      "Epoch 5/10, Batch 1650/1947, Loss: 0.004043972585350275\n",
      "Epoch 5/10, Batch 1700/1947, Loss: 0.0036462012212723494\n",
      "Epoch 5/10, Batch 1750/1947, Loss: 0.004044792149215937\n",
      "Epoch 5/10, Batch 1800/1947, Loss: 0.004218550864607096\n",
      "Epoch 5/10, Batch 1850/1947, Loss: 0.00536277424544096\n",
      "Epoch 5/10, Batch 1900/1947, Loss: 0.004397489130496979\n",
      "New best model saved with val loss: 0.0042\n",
      "\n",
      "Epoch 5 Summary:\n",
      "Train Loss: 0.0042 | Val Loss: 0.0042\n",
      "Epoch 6/10, Batch 0/1947, Loss: 0.003492319257929921\n",
      "Epoch 6/10, Batch 50/1947, Loss: 0.003981199115514755\n",
      "Epoch 6/10, Batch 100/1947, Loss: 0.004192269407212734\n",
      "Epoch 6/10, Batch 150/1947, Loss: 0.0037830264773219824\n",
      "Epoch 6/10, Batch 200/1947, Loss: 0.0037954249419271946\n",
      "Epoch 6/10, Batch 250/1947, Loss: 0.0037623958196491003\n",
      "Epoch 6/10, Batch 300/1947, Loss: 0.004145553335547447\n",
      "Epoch 6/10, Batch 350/1947, Loss: 0.003642550203949213\n",
      "Epoch 6/10, Batch 400/1947, Loss: 0.0036086535546928644\n",
      "Epoch 6/10, Batch 450/1947, Loss: 0.0036861279513686895\n",
      "Epoch 6/10, Batch 500/1947, Loss: 0.0034893937408924103\n",
      "Epoch 6/10, Batch 550/1947, Loss: 0.0034908722154796124\n",
      "Epoch 6/10, Batch 600/1947, Loss: 0.00454819155856967\n",
      "Epoch 6/10, Batch 650/1947, Loss: 0.004403193946927786\n",
      "Epoch 6/10, Batch 700/1947, Loss: 0.004407397471368313\n",
      "Epoch 6/10, Batch 750/1947, Loss: 0.00377881177701056\n",
      "Epoch 6/10, Batch 800/1947, Loss: 0.0038656124379485846\n",
      "Epoch 6/10, Batch 850/1947, Loss: 0.004897452890872955\n",
      "Epoch 6/10, Batch 900/1947, Loss: 0.004424428567290306\n",
      "Epoch 6/10, Batch 950/1947, Loss: 0.004151288419961929\n",
      "Epoch 6/10, Batch 1000/1947, Loss: 0.003901846008375287\n",
      "Epoch 6/10, Batch 1050/1947, Loss: 0.004390882793813944\n",
      "Epoch 6/10, Batch 1100/1947, Loss: 0.003617764450609684\n",
      "Epoch 6/10, Batch 1150/1947, Loss: 0.0038384099025279284\n",
      "Epoch 6/10, Batch 1200/1947, Loss: 0.004197862930595875\n",
      "Epoch 6/10, Batch 1250/1947, Loss: 0.004431036300957203\n",
      "Epoch 6/10, Batch 1300/1947, Loss: 0.004287753254175186\n",
      "Epoch 6/10, Batch 1350/1947, Loss: 0.004234501160681248\n",
      "Epoch 6/10, Batch 1400/1947, Loss: 0.004127661231905222\n",
      "Epoch 6/10, Batch 1450/1947, Loss: 0.004086463246494532\n",
      "Epoch 6/10, Batch 1500/1947, Loss: 0.004481216426938772\n",
      "Epoch 6/10, Batch 1550/1947, Loss: 0.0032360232435166836\n",
      "Epoch 6/10, Batch 1600/1947, Loss: 0.004493684507906437\n",
      "Epoch 6/10, Batch 1650/1947, Loss: 0.004089927766472101\n",
      "Epoch 6/10, Batch 1700/1947, Loss: 0.0038641742430627346\n",
      "Epoch 6/10, Batch 1750/1947, Loss: 0.004558189772069454\n",
      "Epoch 6/10, Batch 1800/1947, Loss: 0.004025484900921583\n",
      "Epoch 6/10, Batch 1850/1947, Loss: 0.00494728097692132\n",
      "Epoch 6/10, Batch 1900/1947, Loss: 0.0038560153916478157\n",
      "New best model saved with val loss: 0.0042\n",
      "\n",
      "Epoch 6 Summary:\n",
      "Train Loss: 0.0041 | Val Loss: 0.0042\n",
      "Epoch 7/10, Batch 0/1947, Loss: 0.0036120968870818615\n",
      "Epoch 7/10, Batch 50/1947, Loss: 0.0040048412047326565\n",
      "Epoch 7/10, Batch 100/1947, Loss: 0.0035992360208183527\n",
      "Epoch 7/10, Batch 150/1947, Loss: 0.004444066900759935\n",
      "Epoch 7/10, Batch 200/1947, Loss: 0.0035941600799560547\n",
      "Epoch 7/10, Batch 250/1947, Loss: 0.004155950155109167\n",
      "Epoch 7/10, Batch 300/1947, Loss: 0.003911586478352547\n",
      "Epoch 7/10, Batch 350/1947, Loss: 0.004079030826687813\n",
      "Epoch 7/10, Batch 400/1947, Loss: 0.0034872929099947214\n",
      "Epoch 7/10, Batch 450/1947, Loss: 0.0039042793214321136\n",
      "Epoch 7/10, Batch 500/1947, Loss: 0.003603140590712428\n",
      "Epoch 7/10, Batch 550/1947, Loss: 0.004272099118679762\n",
      "Epoch 7/10, Batch 600/1947, Loss: 0.004410544876009226\n",
      "Epoch 7/10, Batch 650/1947, Loss: 0.004075120203197002\n",
      "Epoch 7/10, Batch 700/1947, Loss: 0.0038955737836658955\n",
      "Epoch 7/10, Batch 750/1947, Loss: 0.003927256446331739\n",
      "Epoch 7/10, Batch 800/1947, Loss: 0.0038303686305880547\n",
      "Epoch 7/10, Batch 850/1947, Loss: 0.0033409898169338703\n",
      "Epoch 7/10, Batch 900/1947, Loss: 0.0044545140117406845\n",
      "Epoch 7/10, Batch 950/1947, Loss: 0.0042479438707232475\n",
      "Epoch 7/10, Batch 1000/1947, Loss: 0.003510967129841447\n",
      "Epoch 7/10, Batch 1050/1947, Loss: 0.004663507919758558\n",
      "Epoch 7/10, Batch 1100/1947, Loss: 0.0037658896762877703\n",
      "Epoch 7/10, Batch 1150/1947, Loss: 0.004699639044702053\n",
      "Epoch 7/10, Batch 1200/1947, Loss: 0.004275026265531778\n",
      "Epoch 7/10, Batch 1250/1947, Loss: 0.003686711424961686\n",
      "Epoch 7/10, Batch 1300/1947, Loss: 0.00422161677852273\n",
      "Epoch 7/10, Batch 1350/1947, Loss: 0.00424661859869957\n",
      "Epoch 7/10, Batch 1400/1947, Loss: 0.004370492417365313\n",
      "Epoch 7/10, Batch 1450/1947, Loss: 0.0037153856828808784\n",
      "Epoch 7/10, Batch 1500/1947, Loss: 0.003926455974578857\n",
      "Epoch 7/10, Batch 1550/1947, Loss: 0.0038529059384018183\n",
      "Epoch 7/10, Batch 1600/1947, Loss: 0.004548664670437574\n",
      "Epoch 7/10, Batch 1650/1947, Loss: 0.0036718023475259542\n",
      "Epoch 7/10, Batch 1700/1947, Loss: 0.0035665477626025677\n",
      "Epoch 7/10, Batch 1750/1947, Loss: 0.003851174144074321\n",
      "Epoch 7/10, Batch 1800/1947, Loss: 0.004409856628626585\n",
      "Epoch 7/10, Batch 1850/1947, Loss: 0.0036287070252001286\n",
      "Epoch 7/10, Batch 1900/1947, Loss: 0.003739621490240097\n",
      "New best model saved with val loss: 0.0042\n",
      "\n",
      "Epoch 7 Summary:\n",
      "Train Loss: 0.0039 | Val Loss: 0.0042\n",
      "Epoch 8/10, Batch 0/1947, Loss: 0.0033191759139299393\n",
      "Epoch 8/10, Batch 50/1947, Loss: 0.0035919772926717997\n",
      "Epoch 8/10, Batch 100/1947, Loss: 0.0031528323888778687\n",
      "Epoch 8/10, Batch 150/1947, Loss: 0.003901965217664838\n",
      "Epoch 8/10, Batch 200/1947, Loss: 0.0037894516717642546\n",
      "Epoch 8/10, Batch 250/1947, Loss: 0.0036737653426826\n",
      "Epoch 8/10, Batch 300/1947, Loss: 0.0037292398046702147\n",
      "Epoch 8/10, Batch 350/1947, Loss: 0.00425331387668848\n",
      "Epoch 8/10, Batch 400/1947, Loss: 0.003959277179092169\n",
      "Epoch 8/10, Batch 450/1947, Loss: 0.004029505420476198\n",
      "Epoch 8/10, Batch 500/1947, Loss: 0.004032679367810488\n",
      "Epoch 8/10, Batch 550/1947, Loss: 0.004161955788731575\n",
      "Epoch 8/10, Batch 600/1947, Loss: 0.003281070152297616\n",
      "Epoch 8/10, Batch 650/1947, Loss: 0.004107061307877302\n",
      "Epoch 8/10, Batch 700/1947, Loss: 0.0035625973250716925\n",
      "Epoch 8/10, Batch 750/1947, Loss: 0.004245561081916094\n",
      "Epoch 8/10, Batch 800/1947, Loss: 0.0035987028386443853\n",
      "Epoch 8/10, Batch 850/1947, Loss: 0.004714103415608406\n",
      "Epoch 8/10, Batch 900/1947, Loss: 0.003895864821970463\n",
      "Epoch 8/10, Batch 950/1947, Loss: 0.003499074373394251\n",
      "Epoch 8/10, Batch 1000/1947, Loss: 0.0037889117375016212\n",
      "Epoch 8/10, Batch 1050/1947, Loss: 0.004196639638394117\n",
      "Epoch 8/10, Batch 1100/1947, Loss: 0.0037783135194331408\n",
      "Epoch 8/10, Batch 1150/1947, Loss: 0.003532138653099537\n",
      "Epoch 8/10, Batch 1200/1947, Loss: 0.004462820943444967\n",
      "Epoch 8/10, Batch 1250/1947, Loss: 0.0037343092262744904\n",
      "Epoch 8/10, Batch 1300/1947, Loss: 0.004113213159143925\n",
      "Epoch 8/10, Batch 1350/1947, Loss: 0.003742493689060211\n",
      "Epoch 8/10, Batch 1400/1947, Loss: 0.0040791817009449005\n",
      "Epoch 8/10, Batch 1450/1947, Loss: 0.0029206909239292145\n",
      "Epoch 8/10, Batch 1500/1947, Loss: 0.0037665728013962507\n",
      "Epoch 8/10, Batch 1550/1947, Loss: 0.003686106065288186\n",
      "Epoch 8/10, Batch 1600/1947, Loss: 0.004783326759934425\n",
      "Epoch 8/10, Batch 1650/1947, Loss: 0.003369271522387862\n",
      "Epoch 8/10, Batch 1700/1947, Loss: 0.004034009296447039\n",
      "Epoch 8/10, Batch 1750/1947, Loss: 0.00468909228220582\n",
      "Epoch 8/10, Batch 1800/1947, Loss: 0.00358073553070426\n",
      "Epoch 8/10, Batch 1850/1947, Loss: 0.004098712466657162\n",
      "Epoch 8/10, Batch 1900/1947, Loss: 0.0035311318933963776\n",
      "New best model saved with val loss: 0.0041\n",
      "\n",
      "Epoch 8 Summary:\n",
      "Train Loss: 0.0038 | Val Loss: 0.0041\n",
      "Epoch 9/10, Batch 0/1947, Loss: 0.003912655171006918\n",
      "Epoch 9/10, Batch 50/1947, Loss: 0.0036947671324014664\n",
      "Epoch 9/10, Batch 100/1947, Loss: 0.003670510370284319\n",
      "Epoch 9/10, Batch 150/1947, Loss: 0.00311742234043777\n",
      "Epoch 9/10, Batch 200/1947, Loss: 0.003382217837497592\n",
      "Epoch 9/10, Batch 250/1947, Loss: 0.004032901022583246\n",
      "Epoch 9/10, Batch 300/1947, Loss: 0.0037838497664779425\n",
      "Epoch 9/10, Batch 350/1947, Loss: 0.0037453393451869488\n",
      "Epoch 9/10, Batch 400/1947, Loss: 0.0042598857544362545\n",
      "Epoch 9/10, Batch 450/1947, Loss: 0.00334147154353559\n",
      "Epoch 9/10, Batch 500/1947, Loss: 0.003268586006015539\n",
      "Epoch 9/10, Batch 550/1947, Loss: 0.003133495105430484\n",
      "Epoch 9/10, Batch 600/1947, Loss: 0.0037086959928274155\n",
      "Epoch 9/10, Batch 650/1947, Loss: 0.0034603970125317574\n",
      "Epoch 9/10, Batch 700/1947, Loss: 0.0036705192178487778\n",
      "Epoch 9/10, Batch 750/1947, Loss: 0.0032000793144106865\n",
      "Epoch 9/10, Batch 800/1947, Loss: 0.003607557388022542\n",
      "Epoch 9/10, Batch 850/1947, Loss: 0.00394852738827467\n",
      "Epoch 9/10, Batch 900/1947, Loss: 0.0030891047790646553\n",
      "Epoch 9/10, Batch 950/1947, Loss: 0.004297388251870871\n",
      "Epoch 9/10, Batch 1000/1947, Loss: 0.003573262831196189\n",
      "Epoch 9/10, Batch 1050/1947, Loss: 0.003710756544023752\n",
      "Epoch 9/10, Batch 1100/1947, Loss: 0.0033816758077591658\n",
      "Epoch 9/10, Batch 1150/1947, Loss: 0.0037964864168316126\n",
      "Epoch 9/10, Batch 1200/1947, Loss: 0.003613128559663892\n",
      "Epoch 9/10, Batch 1250/1947, Loss: 0.0036140703596174717\n",
      "Epoch 9/10, Batch 1300/1947, Loss: 0.0040063573978841305\n",
      "Epoch 9/10, Batch 1350/1947, Loss: 0.0045599015429615974\n",
      "Epoch 9/10, Batch 1400/1947, Loss: 0.003787583438679576\n",
      "Epoch 9/10, Batch 1450/1947, Loss: 0.00377705879509449\n",
      "Epoch 9/10, Batch 1500/1947, Loss: 0.004079664126038551\n",
      "Epoch 9/10, Batch 1550/1947, Loss: 0.003649880411103368\n",
      "Epoch 9/10, Batch 1600/1947, Loss: 0.003574041649699211\n",
      "Epoch 9/10, Batch 1650/1947, Loss: 0.004445092752575874\n",
      "Epoch 9/10, Batch 1700/1947, Loss: 0.003911820240318775\n",
      "Epoch 9/10, Batch 1750/1947, Loss: 0.0035820873454213142\n",
      "Epoch 9/10, Batch 1800/1947, Loss: 0.0037408575881272554\n",
      "Epoch 9/10, Batch 1850/1947, Loss: 0.004197839647531509\n",
      "Epoch 9/10, Batch 1900/1947, Loss: 0.003968894015997648\n",
      "New best model saved with val loss: 0.0041\n",
      "\n",
      "Epoch 9 Summary:\n",
      "Train Loss: 0.0037 | Val Loss: 0.0041\n",
      "Epoch 10/10, Batch 0/1947, Loss: 0.0036073417868465185\n",
      "Epoch 10/10, Batch 50/1947, Loss: 0.0038298070430755615\n",
      "Epoch 10/10, Batch 100/1947, Loss: 0.00352857680991292\n",
      "Epoch 10/10, Batch 150/1947, Loss: 0.0035703242756426334\n",
      "Epoch 10/10, Batch 200/1947, Loss: 0.004068369045853615\n",
      "Epoch 10/10, Batch 250/1947, Loss: 0.0035907647106796503\n",
      "Epoch 10/10, Batch 300/1947, Loss: 0.0039762649685144424\n",
      "Epoch 10/10, Batch 350/1947, Loss: 0.0032419024500995874\n",
      "Epoch 10/10, Batch 400/1947, Loss: 0.003542295191437006\n",
      "Epoch 10/10, Batch 450/1947, Loss: 0.0037318572867661715\n",
      "Epoch 10/10, Batch 500/1947, Loss: 0.0035125049762427807\n",
      "Epoch 10/10, Batch 550/1947, Loss: 0.0030843864660710096\n",
      "Epoch 10/10, Batch 600/1947, Loss: 0.003761722706258297\n",
      "Epoch 10/10, Batch 650/1947, Loss: 0.004037206992506981\n",
      "Epoch 10/10, Batch 700/1947, Loss: 0.0036144668702036142\n",
      "Epoch 10/10, Batch 750/1947, Loss: 0.0039059901610016823\n",
      "Epoch 10/10, Batch 800/1947, Loss: 0.0038599586114287376\n",
      "Epoch 10/10, Batch 850/1947, Loss: 0.003232441144064069\n",
      "Epoch 10/10, Batch 900/1947, Loss: 0.00378821580670774\n",
      "Epoch 10/10, Batch 950/1947, Loss: 0.003711265977472067\n",
      "Epoch 10/10, Batch 1000/1947, Loss: 0.0032868392299860716\n",
      "Epoch 10/10, Batch 1050/1947, Loss: 0.0035504570696502924\n",
      "Epoch 10/10, Batch 1100/1947, Loss: 0.004026091657578945\n",
      "Epoch 10/10, Batch 1150/1947, Loss: 0.003370505990460515\n",
      "Epoch 10/10, Batch 1200/1947, Loss: 0.003530657384544611\n",
      "Epoch 10/10, Batch 1250/1947, Loss: 0.003655439941212535\n",
      "Epoch 10/10, Batch 1300/1947, Loss: 0.0035830880515277386\n",
      "Epoch 10/10, Batch 1350/1947, Loss: 0.004211282357573509\n",
      "Epoch 10/10, Batch 1400/1947, Loss: 0.0033560232259333134\n",
      "Epoch 10/10, Batch 1450/1947, Loss: 0.0034371118526905775\n",
      "Epoch 10/10, Batch 1500/1947, Loss: 0.003657388733699918\n",
      "Epoch 10/10, Batch 1550/1947, Loss: 0.00314725236967206\n",
      "Epoch 10/10, Batch 1600/1947, Loss: 0.003799180267378688\n",
      "Epoch 10/10, Batch 1650/1947, Loss: 0.0032269242219626904\n",
      "Epoch 10/10, Batch 1700/1947, Loss: 0.0036170801613479853\n",
      "Epoch 10/10, Batch 1750/1947, Loss: 0.003497705329209566\n",
      "Epoch 10/10, Batch 1800/1947, Loss: 0.00400132779031992\n",
      "Epoch 10/10, Batch 1850/1947, Loss: 0.003673162776976824\n",
      "Epoch 10/10, Batch 1900/1947, Loss: 0.003647458739578724\n",
      "\n",
      "Epoch 10 Summary:\n",
      "Train Loss: 0.0036 | Val Loss: 0.0041\n",
      "\n",
      "Training completed!\n",
      "Best validation loss: 0.0041\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training for {num_epochs} epochs started.\")\n",
    "\n",
    "# Initialize variables to track metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0.0\n",
    "\n",
    "    # Training loop\n",
    "    for batch_idx, (data1, data2, data3, targets, _) in enumerate(train_loader):\n",
    "        data1 = data1.to(device)\n",
    "        data2 = data2.to(device)\n",
    "        data3 = data3.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data1, data2, data3)\n",
    "\n",
    "        pos_weight = targets*positive_weigh_factor  # All positive weights are equal to 10\n",
    "        criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item() * data1.size(0)  # Multiply by batch size\n",
    "\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Calculate average training loss for the epoch\n",
    "    epoch_train_loss /= len(train_loader.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data1, data2, data3, targets, _ in val_loader:\n",
    "            data1 = data1.to(device)\n",
    "            data2 = data2.to(device)\n",
    "            data3 = data3.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(data1, data2, data3)\n",
    "            \n",
    "            pos_weight = targets * positive_weigh_factor\n",
    "            criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            epoch_val_loss += loss.item() * data1.size(0)\n",
    "    \n",
    "    # Calculate average validation loss\n",
    "    epoch_val_loss /= len(val_loader.dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    \n",
    "    # Save best model\n",
    "    if epoch_val_loss < best_val_loss:\n",
    "        best_val_loss = epoch_val_loss\n",
    "        torch.save(model.state_dict(), \"best_multimodal-model.pth\")\n",
    "        print(f\"New best model saved with val loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "    print(f\"Train Loss: {epoch_train_loss:.4f} | Val Loss: {epoch_val_loss:.4f}\")\n",
    "\n",
    "# Save the final model\n",
    "torch.save(model.state_dict(), \"final-multimodal-baseline-model.pth\")\n",
    "\n",
    "# Print training summary\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f748985a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load(\"/users/PAS2956/sandeep633/final-multimodal-baseline-model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e8cb49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch #: 0\n",
      "Batch #: 50\n",
      "Batch #: 100\n",
      "Batch #: 150\n",
      "Batch #: 200\n",
      "Batch #: 250\n",
      "Batch #: 300\n",
      "Batch #: 350\n",
      "Batch #: 400\n",
      "Batch #: 450\n",
      "Batch #: 500\n",
      "Batch #: 550\n",
      "Batch #: 600\n",
      "Batch #: 650\n",
      "Batch #: 700\n",
      "Batch #: 750\n",
      "Batch #: 800\n",
      "Batch #: 850\n",
      "Batch #: 900\n",
      "Batch #: 950\n",
      "Batch #: 1000\n",
      "Batch #: 1050\n",
      "Batch #: 1100\n",
      "Batch #: 1150\n",
      "Batch #: 1200\n",
      "Batch #: 1250\n",
      "Batch #: 1300\n",
      "Batch #: 1350\n",
      "Batch #: 1400\n",
      "Batch #: 1450\n",
      "Batch #: 1500\n",
      "Batch #: 1550\n",
      "Batch #: 1600\n",
      "Batch #: 1650\n",
      "Batch #: 1700\n",
      "Batch #: 1750\n",
      "Batch #: 1800\n",
      "Batch #: 1850\n",
      "Batch #: 1900\n",
      "Batch #: 1950\n",
      "Batch #: 2000\n",
      "Batch #: 2050\n",
      "Batch #: 2100\n",
      "Batch #: 2150\n",
      "Batch #: 2200\n",
      "Batch #: 2250\n",
      "Batch #: 2300\n",
      "Batch #: 2350\n",
      "Batch #: 2400\n",
      "Batch #: 2450\n",
      "Batch #: 2500\n",
      "Batch #: 2550\n",
      "Batch #: 2600\n",
      "Batch #: 2650\n",
      "Batch #: 2700\n",
      "Batch #: 2750\n",
      "Batch #: 2800\n",
      "Batch #: 2850\n",
      "Batch #: 2900\n",
      "Batch #: 2950\n",
      "Batch #: 3000\n",
      "Batch #: 3050\n",
      "Batch #: 3100\n",
      "Batch #: 3150\n",
      "Batch #: 3200\n",
      "Batch #: 3250\n",
      "Batch #: 3300\n",
      "Batch #: 3350\n",
      "Batch #: 3400\n",
      "Batch #: 3450\n",
      "Batch #: 3500\n",
      "Batch #: 3550\n",
      "Batch #: 3600\n",
      "Batch #: 3650\n",
      "Batch #: 3700\n",
      "Batch #: 3750\n",
      "Batch #: 3800\n",
      "Batch #: 3850\n",
      "Batch #: 3900\n",
      "Batch #: 3950\n",
      "Batch #: 4000\n",
      "Batch #: 4050\n",
      "Batch #: 4100\n",
      "Batch #: 4150\n",
      "Batch #: 4200\n",
      "Batch #: 4250\n",
      "Batch #: 4300\n",
      "Batch #: 4350\n",
      "Batch #: 4400\n",
      "Batch #: 4450\n",
      "Batch #: 4500\n",
      "Batch #: 4550\n",
      "Batch #: 4600\n",
      "Batch #: 4650\n",
      "Batch #: 4700\n",
      "Batch #: 4750\n",
      "Batch #: 4800\n",
      "Batch #: 4850\n",
      "Batch #: 4900\n",
      "Batch #: 4950\n",
      "Batch #: 5000\n",
      "Batch #: 5050\n",
      "Batch #: 5100\n",
      "Batch #: 5150\n",
      "Batch #: 5200\n",
      "Batch #: 5250\n",
      "Batch #: 5300\n",
      "Batch #: 5350\n",
      "Batch #: 5400\n",
      "Batch #: 5450\n",
      "Batch #: 5500\n",
      "Batch #: 5550\n",
      "Batch #: 5600\n",
      "Batch #: 5650\n",
      "Batch #: 5700\n",
      "Batch #: 5750\n",
      "Batch #: 5800\n",
      "Batch #: 5850\n",
      "Batch #: 5900\n",
      "Batch #: 5950\n",
      "Batch #: 6000\n",
      "Batch #: 6050\n",
      "Batch #: 6100\n",
      "Batch #: 6150\n",
      "Batch #: 6200\n",
      "Batch #: 6250\n",
      "Batch #: 6300\n",
      "Batch #: 6350\n",
      "Batch #: 6400\n",
      "Batch #: 6450\n",
      "Batch #: 6500\n",
      "Batch #: 6550\n",
      "Batch #: 6600\n",
      "Batch #: 6650\n",
      "Batch #: 6700\n",
      "Batch #: 6750\n",
      "Batch #: 6800\n",
      "Batch #: 6850\n",
      "Batch #: 6900\n",
      "Batch #: 6950\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    all_predictions = []\n",
    "    surveys = []\n",
    "    top_k_indices = None\n",
    "    for batch_idx, (data1, data2, data3, surveyID) in enumerate(test_loader):\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f\"Batch #: {batch_idx}\")\n",
    "        data1 = data1.to(device)\n",
    "        data2 = data2.to(device)\n",
    "        data3 = data3.to(device)\n",
    "\n",
    "        outputs = model(data1, data2, data3)\n",
    "        predictions = torch.sigmoid(outputs).cpu().numpy()\n",
    "\n",
    "        # Sellect top-25 values as predictions\n",
    "        top_25 = np.argsort(-predictions, axis=1)[:, :25] \n",
    "        if top_k_indices is None:\n",
    "            top_k_indices = top_25\n",
    "        else:\n",
    "            top_k_indices = np.concatenate((top_k_indices, top_25), axis=0)\n",
    "\n",
    "        surveys.extend([int(sid) for sid in surveyID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe337f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_concatenated = [' '.join(map(str, row)) for row in top_k_indices]\n",
    "\n",
    "pd.DataFrame(\n",
    "    {'surveyId': surveys,\n",
    "     'predictions': data_concatenated,\n",
    "    }).to_csv(\"Final_Multimodal_Baseline.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6dad906",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_predicted = pd.DataFrame(\n",
    "    {'surveyId': surveys,\n",
    "     'predictions': data_concatenated,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bad834fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Accuracy (is true species in top-25?): 0.4336\n",
      "Micro F1-score: 0.3416\n",
      "Macro F1-score: 0.0852\n"
     ]
    }
   ],
   "source": [
    "evaluate_predictions(baseline_predicted, test_metadata, top_k=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7aba7b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Sentinel Predictions\n",
    "baseline_sentinel_only_path = \"/fs/scratch/PAS2985/group_23/baseline_submission.csv\"\n",
    "\n",
    "baseline_sentinel_only = pd.read_csv(baseline_sentinel_only_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d104aefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Accuracy (is true species in top-25?): 0.3573\n",
      "Micro F1-score: 0.2812\n",
      "Macro F1-score: 0.1020\n"
     ]
    }
   ],
   "source": [
    "evaluate_predictions(baseline_sentinel_only, test_metadata, top_k=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4bdbfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d1d7f1b",
   "metadata": {},
   "source": [
    "## Advanced model-3 (Resnet 50, Resnet 50, Swin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "007b1b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedMultimodalEnsemble(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(AdvancedMultimodalEnsemble, self).__init__()\n",
    "        \n",
    "        # Landsat Branch - Using EfficientNet with attention\n",
    "        self.landsat_norm = nn.LayerNorm([6,4,21])\n",
    "        self.landsat_model = models.resnext50_32x4d(weights=None)\n",
    "        self.landsat_model.conv1 = nn.Conv2d(6, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.landsat_model.maxpool = nn.Identity()\n",
    "        self.landsat_model.fc = nn.Identity()\n",
    "        \n",
    "        # Bioclim Branch - Using ResNeXt with squeeze-excitation\n",
    "        self.bioclim_norm = nn.LayerNorm([4,19,12])\n",
    "        self.bioclim_model = models.resnext50_32x4d(weights=None)\n",
    "        self.bioclim_model.conv1 = nn.Conv2d(4, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bioclim_model.maxpool = nn.Identity()\n",
    "        self.bioclim_model.fc = nn.Identity()\n",
    "        \n",
    "        # Sentinel Branch - Using Swin Transformer v2\n",
    "        self.sentinel_resize = nn.Upsample(size=(224, 224), mode='bilinear')\n",
    "        self.sentinel_model = SwinModel.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
    "                # Custom patch embedding for 64x64 inputs\n",
    "        self.sentinel_model.embeddings.patch_embeddings.projection = nn.Conv2d(4, 96, kernel_size=(4, 4), stride=(4, 4))\n",
    "        \n",
    "        # Cross-modal attention mechanism\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=512, num_heads=8)\n",
    "        \n",
    "        # Feature processing\n",
    "        self.landsat_fc = nn.Linear(2048, 512)\n",
    "        self.bioclim_fc = nn.Linear(2048, 512)\n",
    "        self.sentinel_fc = nn.Linear(768, 512)\n",
    "        \n",
    "        # Final classifier with residual connections\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1536, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, y, z):\n",
    "        # Landsat processing\n",
    "        x = self.landsat_norm(x)\n",
    "        x = self.landsat_model(x)\n",
    "        x = self.landsat_fc(x)\n",
    "        \n",
    "        # Bioclim processing\n",
    "        y = self.bioclim_norm(y)\n",
    "        y = self.bioclim_model(y)\n",
    "        y = self.bioclim_fc(y)\n",
    "        \n",
    "        # Sentinel processing\n",
    "        z = self.sentinel_resize(z)  # [32, 4, 224, 224]\n",
    "        z = self.sentinel_model(z).last_hidden_state.mean(dim=1)\n",
    "        z = self.sentinel_fc(z)\n",
    "        \n",
    "        # Cross-modal attention\n",
    "        combined = torch.stack([x, y, z], dim=1)  # [batch, 3, 512]\n",
    "        attn_output, _ = self.cross_attention(combined, combined, combined)\n",
    "        attn_output = attn_output.reshape(attn_output.shape[0], -1)\n",
    "        \n",
    "        # Final classification\n",
    "        out = self.classifier(attn_output)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70647769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    # Set seed for Python's built-in random number generator\n",
    "    torch.manual_seed(seed)\n",
    "    # Set seed for numpy\n",
    "    np.random.seed(seed)\n",
    "    # Set seed for CUDA if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # Set cuDNN's random number generator seed for deterministic behavior\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20f7ce2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = CUDA\n"
     ]
    }
   ],
   "source": [
    "# Check if cuda is available\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"DEVICE = CUDA\")\n",
    "    \n",
    "num_classes = 11255\n",
    "advanced_model = AdvancedMultimodalEnsemble(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55d943ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 10\n",
    "positive_weigh_factor = 1.0\n",
    "num_classes = 11255\n",
    "\n",
    "optimizer = torch.optim.AdamW(advanced_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08c85aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10 epochs started.\n",
      "Epoch 1/10, Batch 0/1947, Loss: 0.9744227528572083\n",
      "Epoch 1/10, Batch 50/1947, Loss: 0.8596339225769043\n",
      "Epoch 1/10, Batch 100/1947, Loss: 0.7834423780441284\n",
      "Epoch 1/10, Batch 150/1947, Loss: 0.7453606128692627\n",
      "Epoch 1/10, Batch 200/1947, Loss: 0.740249514579773\n",
      "Epoch 1/10, Batch 250/1947, Loss: 0.7184439301490784\n",
      "Epoch 1/10, Batch 300/1947, Loss: 0.7129737138748169\n",
      "Epoch 1/10, Batch 350/1947, Loss: 0.7057222127914429\n",
      "Epoch 1/10, Batch 400/1947, Loss: 0.7045723795890808\n",
      "Epoch 1/10, Batch 450/1947, Loss: 0.7021629214286804\n",
      "Epoch 1/10, Batch 500/1947, Loss: 0.7021082043647766\n",
      "Epoch 1/10, Batch 550/1947, Loss: 0.6993110775947571\n",
      "Epoch 1/10, Batch 600/1947, Loss: 0.6981902122497559\n",
      "Epoch 1/10, Batch 650/1947, Loss: 0.6975571513175964\n",
      "Epoch 1/10, Batch 700/1947, Loss: 0.6983944177627563\n",
      "Epoch 1/10, Batch 750/1947, Loss: 0.6965088844299316\n",
      "Epoch 1/10, Batch 800/1947, Loss: 0.6963577270507812\n",
      "Epoch 1/10, Batch 850/1947, Loss: 0.6961405873298645\n",
      "Epoch 1/10, Batch 900/1947, Loss: 0.6957855224609375\n",
      "Epoch 1/10, Batch 950/1947, Loss: 0.6955219507217407\n",
      "Epoch 1/10, Batch 1000/1947, Loss: 0.6956952214241028\n",
      "Epoch 1/10, Batch 1050/1947, Loss: 0.6951919794082642\n",
      "Epoch 1/10, Batch 1100/1947, Loss: 0.6950403451919556\n",
      "Epoch 1/10, Batch 1150/1947, Loss: 0.6947852373123169\n",
      "Epoch 1/10, Batch 1200/1947, Loss: 0.6946377754211426\n",
      "Epoch 1/10, Batch 1250/1947, Loss: 0.6953840851783752\n",
      "Epoch 1/10, Batch 1300/1947, Loss: 0.6943263411521912\n",
      "Epoch 1/10, Batch 1350/1947, Loss: 0.6941930651664734\n",
      "Epoch 1/10, Batch 1400/1947, Loss: 0.6942686438560486\n",
      "Epoch 1/10, Batch 1450/1947, Loss: 0.6941437125205994\n",
      "Epoch 1/10, Batch 1500/1947, Loss: 0.6941685676574707\n",
      "Epoch 1/10, Batch 1550/1947, Loss: 0.6942930817604065\n",
      "Epoch 1/10, Batch 1600/1947, Loss: 0.6939615607261658\n",
      "Epoch 1/10, Batch 1650/1947, Loss: 0.6942662596702576\n",
      "Epoch 1/10, Batch 1700/1947, Loss: 0.693864107131958\n",
      "Epoch 1/10, Batch 1750/1947, Loss: 0.6938658356666565\n",
      "Epoch 1/10, Batch 1800/1947, Loss: 0.6937844753265381\n",
      "Epoch 1/10, Batch 1850/1947, Loss: 0.6937627792358398\n",
      "Epoch 1/10, Batch 1900/1947, Loss: 0.6937381625175476\n",
      "New advanced best model saved with val loss: 0.6937\n",
      "\n",
      "Epoch 1 Summary:\n",
      "Train Loss: 0.7097 | Val Loss: 0.6937\n",
      "Epoch 2/10, Batch 0/1947, Loss: 0.6938101053237915\n",
      "Epoch 2/10, Batch 50/1947, Loss: 0.6936164498329163\n",
      "Epoch 2/10, Batch 100/1947, Loss: 0.6936286091804504\n",
      "Epoch 2/10, Batch 150/1947, Loss: 0.6935943365097046\n",
      "Epoch 2/10, Batch 200/1947, Loss: 0.6936596035957336\n",
      "Epoch 2/10, Batch 250/1947, Loss: 0.6935336589813232\n",
      "Epoch 2/10, Batch 300/1947, Loss: 0.6935597062110901\n",
      "Epoch 2/10, Batch 350/1947, Loss: 0.6935639977455139\n",
      "Epoch 2/10, Batch 400/1947, Loss: 0.6938068866729736\n",
      "Epoch 2/10, Batch 450/1947, Loss: 0.6936766505241394\n",
      "Epoch 2/10, Batch 500/1947, Loss: 0.6939058899879456\n",
      "Epoch 2/10, Batch 550/1947, Loss: 0.6935228109359741\n",
      "Epoch 2/10, Batch 600/1947, Loss: 0.6934577226638794\n",
      "Epoch 2/10, Batch 650/1947, Loss: 0.6934954524040222\n",
      "Epoch 2/10, Batch 700/1947, Loss: 0.6934108138084412\n",
      "Epoch 2/10, Batch 750/1947, Loss: 0.693394124507904\n",
      "Epoch 2/10, Batch 800/1947, Loss: 0.6934100985527039\n",
      "Epoch 2/10, Batch 850/1947, Loss: 0.6934494972229004\n",
      "Epoch 2/10, Batch 900/1947, Loss: 0.6933836340904236\n",
      "Epoch 2/10, Batch 950/1947, Loss: 0.6933807134628296\n",
      "Epoch 2/10, Batch 1000/1947, Loss: 0.6934763789176941\n",
      "Epoch 2/10, Batch 1050/1947, Loss: 0.6933693885803223\n",
      "Epoch 2/10, Batch 1100/1947, Loss: 0.693382203578949\n",
      "Epoch 2/10, Batch 1150/1947, Loss: 0.6933858394622803\n",
      "Epoch 2/10, Batch 1200/1947, Loss: 0.6933402419090271\n",
      "Epoch 2/10, Batch 1250/1947, Loss: 0.6933404803276062\n",
      "Epoch 2/10, Batch 1300/1947, Loss: 0.6933327913284302\n",
      "Epoch 2/10, Batch 1350/1947, Loss: 0.6933357119560242\n",
      "Epoch 2/10, Batch 1400/1947, Loss: 0.6933359503746033\n",
      "Epoch 2/10, Batch 1450/1947, Loss: 0.693295955657959\n",
      "Epoch 2/10, Batch 1500/1947, Loss: 0.6932972073554993\n",
      "Epoch 2/10, Batch 1550/1947, Loss: 0.6932867169380188\n",
      "Epoch 2/10, Batch 1600/1947, Loss: 0.6932805180549622\n",
      "Epoch 2/10, Batch 1650/1947, Loss: 0.6932860612869263\n",
      "Epoch 2/10, Batch 1700/1947, Loss: 0.6932703852653503\n",
      "Epoch 2/10, Batch 1750/1947, Loss: 0.69331955909729\n",
      "Epoch 2/10, Batch 1800/1947, Loss: 0.6932719349861145\n",
      "Epoch 2/10, Batch 1850/1947, Loss: 0.6933348774909973\n",
      "Epoch 2/10, Batch 1900/1947, Loss: 0.6932570338249207\n",
      "New advanced best model saved with val loss: 0.6933\n",
      "\n",
      "Epoch 2 Summary:\n",
      "Train Loss: 0.6934 | Val Loss: 0.6933\n",
      "Epoch 3/10, Batch 0/1947, Loss: 0.6932680606842041\n",
      "Epoch 3/10, Batch 50/1947, Loss: 0.6932580471038818\n",
      "Epoch 3/10, Batch 100/1947, Loss: 0.6932477951049805\n",
      "Epoch 3/10, Batch 150/1947, Loss: 0.6932373046875\n",
      "Epoch 3/10, Batch 200/1947, Loss: 0.6932500004768372\n",
      "Epoch 3/10, Batch 250/1947, Loss: 0.6932482719421387\n",
      "Epoch 3/10, Batch 300/1947, Loss: 0.6932475566864014\n",
      "Epoch 3/10, Batch 350/1947, Loss: 0.6933320760726929\n",
      "Epoch 3/10, Batch 400/1947, Loss: 0.6932479739189148\n",
      "Epoch 3/10, Batch 450/1947, Loss: 0.6932980418205261\n",
      "Epoch 3/10, Batch 500/1947, Loss: 0.6932287216186523\n",
      "Epoch 3/10, Batch 550/1947, Loss: 0.6932734847068787\n",
      "Epoch 3/10, Batch 600/1947, Loss: 0.6932146549224854\n",
      "Epoch 3/10, Batch 650/1947, Loss: 0.6932337880134583\n",
      "Epoch 3/10, Batch 700/1947, Loss: 0.6932163834571838\n",
      "Epoch 3/10, Batch 750/1947, Loss: 0.6932123899459839\n",
      "Epoch 3/10, Batch 800/1947, Loss: 0.6932119131088257\n",
      "Epoch 3/10, Batch 850/1947, Loss: 0.6932091116905212\n",
      "Epoch 3/10, Batch 900/1947, Loss: 0.6932129859924316\n",
      "Epoch 3/10, Batch 950/1947, Loss: 0.693204402923584\n",
      "Epoch 3/10, Batch 1000/1947, Loss: 0.6932042837142944\n",
      "Epoch 3/10, Batch 1050/1947, Loss: 0.6932010650634766\n",
      "Epoch 3/10, Batch 1100/1947, Loss: 0.6931991577148438\n",
      "Epoch 3/10, Batch 1150/1947, Loss: 0.6931962966918945\n",
      "Epoch 3/10, Batch 1200/1947, Loss: 0.6931921243667603\n",
      "Epoch 3/10, Batch 1250/1947, Loss: 0.6931948065757751\n",
      "Epoch 3/10, Batch 1300/1947, Loss: 0.6932015419006348\n",
      "Epoch 3/10, Batch 1350/1947, Loss: 0.693196177482605\n",
      "Epoch 3/10, Batch 1400/1947, Loss: 0.6932048201560974\n",
      "Epoch 3/10, Batch 1450/1947, Loss: 0.6931971907615662\n",
      "Epoch 3/10, Batch 1500/1947, Loss: 0.6931893229484558\n",
      "Epoch 3/10, Batch 1550/1947, Loss: 0.6932054162025452\n",
      "Epoch 3/10, Batch 1600/1947, Loss: 0.6931986808776855\n",
      "Epoch 3/10, Batch 1650/1947, Loss: 0.6931910514831543\n",
      "Epoch 3/10, Batch 1700/1947, Loss: 0.6931838393211365\n",
      "Epoch 3/10, Batch 1750/1947, Loss: 0.6933199763298035\n",
      "Epoch 3/10, Batch 1800/1947, Loss: 0.693242609500885\n",
      "Epoch 3/10, Batch 1850/1947, Loss: 0.6931988596916199\n",
      "Epoch 3/10, Batch 1900/1947, Loss: 0.6931837201118469\n",
      "New advanced best model saved with val loss: 0.6932\n",
      "\n",
      "Epoch 3 Summary:\n",
      "Train Loss: 0.6932 | Val Loss: 0.6932\n",
      "Epoch 4/10, Batch 0/1947, Loss: 0.6931776404380798\n",
      "Epoch 4/10, Batch 50/1947, Loss: 0.693179190158844\n",
      "Epoch 4/10, Batch 100/1947, Loss: 0.6931782960891724\n",
      "Epoch 4/10, Batch 150/1947, Loss: 0.6931789517402649\n",
      "Epoch 4/10, Batch 200/1947, Loss: 0.6931754350662231\n",
      "Epoch 4/10, Batch 250/1947, Loss: 0.6931752562522888\n",
      "Epoch 4/10, Batch 300/1947, Loss: 0.6931766867637634\n",
      "Epoch 4/10, Batch 350/1947, Loss: 0.6931725740432739\n",
      "Epoch 4/10, Batch 400/1947, Loss: 0.6931736469268799\n",
      "Epoch 4/10, Batch 450/1947, Loss: 0.6931700110435486\n",
      "Epoch 4/10, Batch 500/1947, Loss: 0.6931697130203247\n",
      "Epoch 4/10, Batch 550/1947, Loss: 0.6931765675544739\n",
      "Epoch 4/10, Batch 600/1947, Loss: 0.6931706666946411\n",
      "Epoch 4/10, Batch 650/1947, Loss: 0.6931677460670471\n",
      "Epoch 4/10, Batch 700/1947, Loss: 0.6931674480438232\n",
      "Epoch 4/10, Batch 750/1947, Loss: 0.6931758522987366\n",
      "Epoch 4/10, Batch 800/1947, Loss: 0.6931684613227844\n",
      "Epoch 4/10, Batch 850/1947, Loss: 0.693166196346283\n",
      "Epoch 4/10, Batch 900/1947, Loss: 0.6931692957878113\n",
      "Epoch 4/10, Batch 950/1947, Loss: 0.6931658983230591\n",
      "Epoch 4/10, Batch 1000/1947, Loss: 0.6931701302528381\n",
      "Epoch 4/10, Batch 1050/1947, Loss: 0.6931670308113098\n",
      "Epoch 4/10, Batch 1100/1947, Loss: 0.6931623220443726\n",
      "Epoch 4/10, Batch 1150/1947, Loss: 0.6931652426719666\n",
      "Epoch 4/10, Batch 1200/1947, Loss: 0.6931664943695068\n",
      "Epoch 4/10, Batch 1250/1947, Loss: 0.6931635737419128\n",
      "Epoch 4/10, Batch 1300/1947, Loss: 0.6931644082069397\n",
      "Epoch 4/10, Batch 1350/1947, Loss: 0.693162202835083\n",
      "Epoch 4/10, Batch 1400/1947, Loss: 0.6931666731834412\n",
      "Epoch 4/10, Batch 1450/1947, Loss: 0.6931612491607666\n",
      "Epoch 4/10, Batch 1500/1947, Loss: 0.6931605935096741\n",
      "Epoch 4/10, Batch 1550/1947, Loss: 0.6931624412536621\n",
      "Epoch 4/10, Batch 1600/1947, Loss: 0.6931615471839905\n",
      "Epoch 4/10, Batch 1650/1947, Loss: 0.6931594610214233\n",
      "Epoch 4/10, Batch 1700/1947, Loss: 0.6931593418121338\n",
      "Epoch 4/10, Batch 1750/1947, Loss: 0.6931648254394531\n",
      "Epoch 4/10, Batch 1800/1947, Loss: 0.6931613683700562\n",
      "Epoch 4/10, Batch 1850/1947, Loss: 0.693159818649292\n",
      "Epoch 4/10, Batch 1900/1947, Loss: 0.6931613683700562\n",
      "New advanced best model saved with val loss: 0.6932\n",
      "\n",
      "Epoch 4 Summary:\n",
      "Train Loss: 0.6932 | Val Loss: 0.6932\n",
      "Epoch 5/10, Batch 0/1947, Loss: 0.6931602954864502\n",
      "Epoch 5/10, Batch 50/1947, Loss: 0.6931625008583069\n",
      "Epoch 5/10, Batch 100/1947, Loss: 0.6931567788124084\n",
      "Epoch 5/10, Batch 150/1947, Loss: 0.6931594014167786\n",
      "Epoch 5/10, Batch 200/1947, Loss: 0.6931578516960144\n",
      "Epoch 5/10, Batch 250/1947, Loss: 0.6931576132774353\n",
      "Epoch 5/10, Batch 300/1947, Loss: 0.6931559443473816\n",
      "Epoch 5/10, Batch 350/1947, Loss: 0.6931591629981995\n",
      "Epoch 5/10, Batch 400/1947, Loss: 0.6931558847427368\n",
      "Epoch 5/10, Batch 450/1947, Loss: 0.693156361579895\n",
      "Epoch 5/10, Batch 500/1947, Loss: 0.6931561231613159\n",
      "Epoch 5/10, Batch 550/1947, Loss: 0.6931557655334473\n",
      "Epoch 5/10, Batch 600/1947, Loss: 0.693155825138092\n",
      "Epoch 5/10, Batch 650/1947, Loss: 0.6931552886962891\n",
      "Epoch 5/10, Batch 700/1947, Loss: 0.6931546926498413\n",
      "Epoch 5/10, Batch 750/1947, Loss: 0.6931573152542114\n",
      "Epoch 5/10, Batch 800/1947, Loss: 0.693154513835907\n",
      "Epoch 5/10, Batch 850/1947, Loss: 0.6931543946266174\n",
      "Epoch 5/10, Batch 900/1947, Loss: 0.6931563019752502\n",
      "Epoch 5/10, Batch 950/1947, Loss: 0.6931634545326233\n",
      "Epoch 5/10, Batch 1000/1947, Loss: 0.6931543946266174\n",
      "Epoch 5/10, Batch 1050/1947, Loss: 0.6931554079055786\n",
      "Epoch 5/10, Batch 1100/1947, Loss: 0.6931542754173279\n",
      "Epoch 5/10, Batch 1150/1947, Loss: 0.6931535005569458\n",
      "Epoch 5/10, Batch 1200/1947, Loss: 0.6931564211845398\n",
      "Epoch 5/10, Batch 1250/1947, Loss: 0.6931535601615906\n",
      "Epoch 5/10, Batch 1300/1947, Loss: 0.693153440952301\n",
      "Epoch 5/10, Batch 1350/1947, Loss: 0.6931535601615906\n",
      "Epoch 5/10, Batch 1400/1947, Loss: 0.6931543350219727\n",
      "Epoch 5/10, Batch 1450/1947, Loss: 0.6931540966033936\n",
      "Epoch 5/10, Batch 1500/1947, Loss: 0.6931523084640503\n",
      "Epoch 5/10, Batch 1550/1947, Loss: 0.6931522488594055\n",
      "Epoch 5/10, Batch 1600/1947, Loss: 0.6931536793708801\n",
      "Epoch 5/10, Batch 1650/1947, Loss: 0.6931523680686951\n",
      "Epoch 5/10, Batch 1700/1947, Loss: 0.6931530833244324\n",
      "Epoch 5/10, Batch 1750/1947, Loss: 0.6931536793708801\n",
      "Epoch 5/10, Batch 1800/1947, Loss: 0.6931521892547607\n",
      "Epoch 5/10, Batch 1850/1947, Loss: 0.6931543946266174\n",
      "Epoch 5/10, Batch 1900/1947, Loss: 0.6931520104408264\n",
      "New advanced best model saved with val loss: 0.6932\n",
      "\n",
      "Epoch 5 Summary:\n",
      "Train Loss: 0.6932 | Val Loss: 0.6932\n",
      "Epoch 6/10, Batch 0/1947, Loss: 0.6931524872779846\n",
      "Epoch 6/10, Batch 50/1947, Loss: 0.6931524276733398\n",
      "Epoch 6/10, Batch 100/1947, Loss: 0.6931511163711548\n",
      "Epoch 6/10, Batch 150/1947, Loss: 0.6931513547897339\n",
      "Epoch 6/10, Batch 200/1947, Loss: 0.6931551694869995\n",
      "Epoch 6/10, Batch 250/1947, Loss: 0.6931517124176025\n",
      "Epoch 6/10, Batch 300/1947, Loss: 0.6931511759757996\n",
      "Epoch 6/10, Batch 350/1947, Loss: 0.6931512355804443\n",
      "Epoch 6/10, Batch 400/1947, Loss: 0.6931509375572205\n",
      "Epoch 6/10, Batch 450/1947, Loss: 0.69315105676651\n",
      "Epoch 6/10, Batch 500/1947, Loss: 0.6931511759757996\n",
      "Epoch 6/10, Batch 550/1947, Loss: 0.6931506991386414\n",
      "Epoch 6/10, Batch 600/1947, Loss: 0.6931504607200623\n",
      "Epoch 6/10, Batch 650/1947, Loss: 0.6931509971618652\n",
      "Epoch 6/10, Batch 700/1947, Loss: 0.6931512951850891\n",
      "Epoch 6/10, Batch 750/1947, Loss: 0.693150520324707\n",
      "Epoch 6/10, Batch 800/1947, Loss: 0.6931502223014832\n",
      "Epoch 6/10, Batch 850/1947, Loss: 0.6931502223014832\n",
      "Epoch 6/10, Batch 900/1947, Loss: 0.6931499242782593\n",
      "Epoch 6/10, Batch 950/1947, Loss: 0.6931499242782593\n",
      "Epoch 6/10, Batch 1000/1947, Loss: 0.6931501030921936\n",
      "Epoch 6/10, Batch 1050/1947, Loss: 0.6931506395339966\n",
      "Epoch 6/10, Batch 1100/1947, Loss: 0.6931499242782593\n",
      "Epoch 6/10, Batch 1150/1947, Loss: 0.6931508779525757\n",
      "Epoch 6/10, Batch 1200/1947, Loss: 0.693149745464325\n",
      "Epoch 6/10, Batch 1250/1947, Loss: 0.693149745464325\n",
      "Epoch 6/10, Batch 1300/1947, Loss: 0.693149745464325\n",
      "Epoch 6/10, Batch 1350/1947, Loss: 0.6931496858596802\n",
      "Epoch 6/10, Batch 1400/1947, Loss: 0.6931496858596802\n",
      "Epoch 6/10, Batch 1450/1947, Loss: 0.693149745464325\n",
      "Epoch 6/10, Batch 1500/1947, Loss: 0.6931496858596802\n",
      "Epoch 6/10, Batch 1550/1947, Loss: 0.6931506991386414\n",
      "Epoch 6/10, Batch 1600/1947, Loss: 0.693149983882904\n",
      "Epoch 6/10, Batch 1650/1947, Loss: 0.6931495070457458\n",
      "Epoch 6/10, Batch 1700/1947, Loss: 0.6931492686271667\n",
      "Epoch 6/10, Batch 1750/1947, Loss: 0.6931496262550354\n",
      "Epoch 6/10, Batch 1800/1947, Loss: 0.693149745464325\n",
      "Epoch 6/10, Batch 1850/1947, Loss: 0.6931495666503906\n",
      "Epoch 6/10, Batch 1900/1947, Loss: 0.6931494474411011\n",
      "New advanced best model saved with val loss: 0.6932\n",
      "\n",
      "Epoch 6 Summary:\n",
      "Train Loss: 0.6932 | Val Loss: 0.6932\n",
      "Epoch 7/10, Batch 0/1947, Loss: 0.6931492686271667\n",
      "Epoch 7/10, Batch 50/1947, Loss: 0.6931493282318115\n",
      "Epoch 7/10, Batch 100/1947, Loss: 0.6931489109992981\n",
      "Epoch 7/10, Batch 150/1947, Loss: 0.6931490898132324\n",
      "Epoch 7/10, Batch 200/1947, Loss: 0.6931489706039429\n",
      "Epoch 7/10, Batch 250/1947, Loss: 0.6931491494178772\n",
      "Epoch 7/10, Batch 300/1947, Loss: 0.6931490898132324\n",
      "Epoch 7/10, Batch 350/1947, Loss: 0.6931493282318115\n",
      "Epoch 7/10, Batch 400/1947, Loss: 0.6931489706039429\n",
      "Epoch 7/10, Batch 450/1947, Loss: 0.6931490898132324\n",
      "Epoch 7/10, Batch 500/1947, Loss: 0.6931484937667847\n",
      "Epoch 7/10, Batch 550/1947, Loss: 0.6931486129760742\n",
      "Epoch 7/10, Batch 600/1947, Loss: 0.6931489706039429\n",
      "Epoch 7/10, Batch 650/1947, Loss: 0.6931487917900085\n",
      "Epoch 7/10, Batch 700/1947, Loss: 0.6931489109992981\n",
      "Epoch 7/10, Batch 750/1947, Loss: 0.693148672580719\n",
      "Epoch 7/10, Batch 800/1947, Loss: 0.693148672580719\n",
      "Epoch 7/10, Batch 850/1947, Loss: 0.6931487917900085\n",
      "Epoch 7/10, Batch 900/1947, Loss: 0.6931486129760742\n",
      "Epoch 7/10, Batch 950/1947, Loss: 0.6931486129760742\n",
      "Epoch 7/10, Batch 1000/1947, Loss: 0.6931487321853638\n",
      "Epoch 7/10, Batch 1050/1947, Loss: 0.6931487321853638\n",
      "Epoch 7/10, Batch 1100/1947, Loss: 0.6931483149528503\n",
      "Epoch 7/10, Batch 1150/1947, Loss: 0.6931484937667847\n",
      "Epoch 7/10, Batch 1200/1947, Loss: 0.6931483149528503\n",
      "Epoch 7/10, Batch 1250/1947, Loss: 0.6931491494178772\n",
      "Epoch 7/10, Batch 1300/1947, Loss: 0.6931487917900085\n",
      "Epoch 7/10, Batch 1350/1947, Loss: 0.6931483745574951\n",
      "Epoch 7/10, Batch 1400/1947, Loss: 0.693148672580719\n",
      "Epoch 7/10, Batch 1450/1947, Loss: 0.6931500434875488\n",
      "Epoch 7/10, Batch 1500/1947, Loss: 0.6931483745574951\n",
      "Epoch 7/10, Batch 1550/1947, Loss: 0.693148136138916\n",
      "Epoch 7/10, Batch 1600/1947, Loss: 0.693148672580719\n",
      "Epoch 7/10, Batch 1650/1947, Loss: 0.6931481957435608\n",
      "Epoch 7/10, Batch 1700/1947, Loss: 0.6931483149528503\n",
      "Epoch 7/10, Batch 1750/1947, Loss: 0.6931484341621399\n",
      "Epoch 7/10, Batch 1800/1947, Loss: 0.6931483745574951\n",
      "Epoch 7/10, Batch 1850/1947, Loss: 0.6931480169296265\n",
      "Epoch 7/10, Batch 1900/1947, Loss: 0.6931480765342712\n",
      "New advanced best model saved with val loss: 0.6932\n",
      "\n",
      "Epoch 7 Summary:\n",
      "Train Loss: 0.6931 | Val Loss: 0.6932\n",
      "Epoch 8/10, Batch 0/1947, Loss: 0.693148136138916\n",
      "Epoch 8/10, Batch 50/1947, Loss: 0.6931521892547607\n",
      "Epoch 8/10, Batch 100/1947, Loss: 0.6931489109992981\n",
      "Epoch 8/10, Batch 150/1947, Loss: 0.6931584477424622\n",
      "Epoch 8/10, Batch 200/1947, Loss: 0.6931507587432861\n",
      "Epoch 8/10, Batch 250/1947, Loss: 0.6931481957435608\n",
      "Epoch 8/10, Batch 300/1947, Loss: 0.6931480169296265\n",
      "Epoch 8/10, Batch 350/1947, Loss: 0.6931480169296265\n",
      "Epoch 8/10, Batch 400/1947, Loss: 0.6931479573249817\n",
      "Epoch 8/10, Batch 450/1947, Loss: 0.693148136138916\n",
      "Epoch 8/10, Batch 500/1947, Loss: 0.6931478381156921\n",
      "Epoch 8/10, Batch 550/1947, Loss: 0.6931483149528503\n",
      "Epoch 8/10, Batch 600/1947, Loss: 0.6931478381156921\n",
      "Epoch 8/10, Batch 650/1947, Loss: 0.6931480169296265\n",
      "Epoch 8/10, Batch 700/1947, Loss: 0.6931478381156921\n",
      "Epoch 8/10, Batch 750/1947, Loss: 0.6931476593017578\n",
      "Epoch 8/10, Batch 800/1947, Loss: 0.6931477785110474\n",
      "Epoch 8/10, Batch 850/1947, Loss: 0.6931477189064026\n",
      "Epoch 8/10, Batch 900/1947, Loss: 0.6931479573249817\n",
      "Epoch 8/10, Batch 950/1947, Loss: 0.6931476593017578\n",
      "Epoch 8/10, Batch 1000/1947, Loss: 0.6931478381156921\n",
      "Epoch 8/10, Batch 1050/1947, Loss: 0.6931477189064026\n",
      "Epoch 8/10, Batch 1100/1947, Loss: 0.6931476593017578\n",
      "Epoch 8/10, Batch 1150/1947, Loss: 0.6931477785110474\n",
      "Epoch 8/10, Batch 1200/1947, Loss: 0.6931477189064026\n",
      "Epoch 8/10, Batch 1250/1947, Loss: 0.6931478381156921\n",
      "Epoch 8/10, Batch 1300/1947, Loss: 0.6931477785110474\n",
      "Epoch 8/10, Batch 1350/1947, Loss: 0.6931478381156921\n",
      "Epoch 8/10, Batch 1400/1947, Loss: 0.6931476593017578\n",
      "Epoch 8/10, Batch 1450/1947, Loss: 0.6931476593017578\n",
      "Epoch 8/10, Batch 1500/1947, Loss: 0.6931476593017578\n",
      "Epoch 8/10, Batch 1550/1947, Loss: 0.6931475400924683\n",
      "Epoch 8/10, Batch 1600/1947, Loss: 0.6931476593017578\n",
      "Epoch 8/10, Batch 1650/1947, Loss: 0.6931476593017578\n",
      "Epoch 8/10, Batch 1700/1947, Loss: 0.6931476593017578\n",
      "Epoch 8/10, Batch 1750/1947, Loss: 0.693147599697113\n",
      "Epoch 8/10, Batch 1800/1947, Loss: 0.693147599697113\n",
      "Epoch 8/10, Batch 1850/1947, Loss: 0.693147599697113\n",
      "Epoch 8/10, Batch 1900/1947, Loss: 0.6931476593017578\n",
      "New advanced best model saved with val loss: 0.6931\n",
      "\n",
      "Epoch 8 Summary:\n",
      "Train Loss: 0.6931 | Val Loss: 0.6931\n",
      "Epoch 9/10, Batch 0/1947, Loss: 0.693147599697113\n",
      "Epoch 9/10, Batch 50/1947, Loss: 0.6931475400924683\n",
      "Epoch 9/10, Batch 100/1947, Loss: 0.6931476593017578\n",
      "Epoch 9/10, Batch 150/1947, Loss: 0.6931477189064026\n",
      "Epoch 9/10, Batch 200/1947, Loss: 0.6931475400924683\n",
      "Epoch 9/10, Batch 250/1947, Loss: 0.6931475400924683\n",
      "Epoch 9/10, Batch 300/1947, Loss: 0.6931475400924683\n",
      "Epoch 9/10, Batch 350/1947, Loss: 0.6931477785110474\n",
      "Epoch 9/10, Batch 400/1947, Loss: 0.6931475400924683\n",
      "Epoch 9/10, Batch 450/1947, Loss: 0.693147599697113\n",
      "Epoch 9/10, Batch 500/1947, Loss: 0.6931474804878235\n",
      "Epoch 9/10, Batch 550/1947, Loss: 0.6931475400924683\n",
      "Epoch 9/10, Batch 600/1947, Loss: 0.6931475400924683\n",
      "Epoch 9/10, Batch 650/1947, Loss: 0.6931475400924683\n",
      "Epoch 9/10, Batch 700/1947, Loss: 0.6931475400924683\n",
      "Epoch 9/10, Batch 750/1947, Loss: 0.6931486129760742\n",
      "Epoch 9/10, Batch 800/1947, Loss: 0.6931475400924683\n",
      "Epoch 9/10, Batch 850/1947, Loss: 0.6931475400924683\n",
      "Epoch 9/10, Batch 900/1947, Loss: 0.6931477189064026\n",
      "Epoch 9/10, Batch 950/1947, Loss: 0.693147599697113\n",
      "Epoch 9/10, Batch 1000/1947, Loss: 0.6931475400924683\n",
      "Epoch 9/10, Batch 1050/1947, Loss: 0.6931475400924683\n",
      "Epoch 9/10, Batch 1100/1947, Loss: 0.6931475400924683\n",
      "Epoch 9/10, Batch 1150/1947, Loss: 0.6931474804878235\n",
      "Epoch 9/10, Batch 1200/1947, Loss: 0.6931474804878235\n",
      "Epoch 9/10, Batch 1250/1947, Loss: 0.6931474208831787\n",
      "Epoch 9/10, Batch 1300/1947, Loss: 0.6931477189064026\n",
      "Epoch 9/10, Batch 1350/1947, Loss: 0.6931474804878235\n",
      "Epoch 9/10, Batch 1400/1947, Loss: 0.6931474804878235\n",
      "Epoch 9/10, Batch 1450/1947, Loss: 0.6931474208831787\n",
      "Epoch 9/10, Batch 1500/1947, Loss: 0.6931474804878235\n",
      "Epoch 9/10, Batch 1550/1947, Loss: 0.6931474208831787\n",
      "Epoch 9/10, Batch 1600/1947, Loss: 0.6931475400924683\n",
      "Epoch 9/10, Batch 1650/1947, Loss: 0.6931474208831787\n",
      "Epoch 9/10, Batch 1700/1947, Loss: 0.6931473612785339\n",
      "Epoch 9/10, Batch 1750/1947, Loss: 0.6931473612785339\n",
      "Epoch 9/10, Batch 1800/1947, Loss: 0.6931474208831787\n",
      "Epoch 9/10, Batch 1850/1947, Loss: 0.6931473612785339\n",
      "Epoch 9/10, Batch 1900/1947, Loss: 0.6931474208831787\n",
      "New advanced best model saved with val loss: 0.6931\n",
      "\n",
      "Epoch 9 Summary:\n",
      "Train Loss: 0.6931 | Val Loss: 0.6931\n",
      "Epoch 10/10, Batch 0/1947, Loss: 0.6931473612785339\n",
      "Epoch 10/10, Batch 50/1947, Loss: 0.6931474208831787\n",
      "Epoch 10/10, Batch 100/1947, Loss: 0.6931473612785339\n",
      "Epoch 10/10, Batch 150/1947, Loss: 0.6931474208831787\n",
      "Epoch 10/10, Batch 200/1947, Loss: 0.6931474208831787\n",
      "Epoch 10/10, Batch 250/1947, Loss: 0.6931474804878235\n",
      "Epoch 10/10, Batch 300/1947, Loss: 0.6931474208831787\n",
      "Epoch 10/10, Batch 350/1947, Loss: 0.6931474208831787\n",
      "Epoch 10/10, Batch 400/1947, Loss: 0.6931473612785339\n",
      "Epoch 10/10, Batch 450/1947, Loss: 0.6931474208831787\n",
      "Epoch 10/10, Batch 500/1947, Loss: 0.6931473612785339\n",
      "Epoch 10/10, Batch 550/1947, Loss: 0.6931473612785339\n",
      "Epoch 10/10, Batch 600/1947, Loss: 0.6931474208831787\n",
      "Epoch 10/10, Batch 650/1947, Loss: 0.6931473612785339\n",
      "Epoch 10/10, Batch 700/1947, Loss: 0.6931473612785339\n",
      "Epoch 10/10, Batch 750/1947, Loss: 0.6931473612785339\n",
      "Epoch 10/10, Batch 800/1947, Loss: 0.6931473612785339\n",
      "Epoch 10/10, Batch 850/1947, Loss: 0.6931473612785339\n",
      "Epoch 10/10, Batch 900/1947, Loss: 0.6931473612785339\n",
      "Epoch 10/10, Batch 950/1947, Loss: 0.6931473612785339\n",
      "Epoch 10/10, Batch 1000/1947, Loss: 0.6931473612785339\n",
      "Epoch 10/10, Batch 1050/1947, Loss: 0.6931473612785339\n",
      "Epoch 10/10, Batch 1100/1947, Loss: 0.6931473612785339\n",
      "Epoch 10/10, Batch 1150/1947, Loss: 0.6931473612785339\n",
      "Epoch 10/10, Batch 1200/1947, Loss: 0.6931473612785339\n",
      "Epoch 10/10, Batch 1250/1947, Loss: 0.6931473612785339\n",
      "Epoch 10/10, Batch 1300/1947, Loss: 0.6931473612785339\n",
      "Epoch 10/10, Batch 1350/1947, Loss: 0.6931473612785339\n",
      "Epoch 10/10, Batch 1400/1947, Loss: 0.6931473612785339\n",
      "Epoch 10/10, Batch 1450/1947, Loss: 0.6931473612785339\n",
      "Epoch 10/10, Batch 1500/1947, Loss: 0.6931474208831787\n",
      "Epoch 10/10, Batch 1550/1947, Loss: 0.6931473612785339\n",
      "Epoch 10/10, Batch 1600/1947, Loss: 0.6931473612785339\n",
      "Epoch 10/10, Batch 1650/1947, Loss: 0.6931473612785339\n",
      "Epoch 10/10, Batch 1700/1947, Loss: 0.6931473612785339\n",
      "Epoch 10/10, Batch 1750/1947, Loss: 0.6931473612785339\n",
      "Epoch 10/10, Batch 1800/1947, Loss: 0.6931473612785339\n",
      "Epoch 10/10, Batch 1850/1947, Loss: 0.6931473612785339\n",
      "Epoch 10/10, Batch 1900/1947, Loss: 0.6931473612785339\n",
      "New advanced best model saved with val loss: 0.6931\n",
      "\n",
      "Epoch 10 Summary:\n",
      "Train Loss: 0.6931 | Val Loss: 0.6931\n",
      "\n",
      "Training completed!\n",
      "Best validation loss: 0.6931\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training for {num_epochs} epochs started.\")\n",
    "\n",
    "# Initialize variables to track metrics\n",
    "adv_train_losses = []\n",
    "adv_val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    advanced_model.train()\n",
    "    epoch_train_loss = 0.0\n",
    "\n",
    "    # Training loop\n",
    "    for batch_idx, (data1, data2, data3, targets, _) in enumerate(train_loader):\n",
    "        data1 = data1.to(device)\n",
    "        data2 = data2.to(device)\n",
    "        data3 = data3.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = advanced_model(data1, data2, data3)\n",
    "\n",
    "        pos_weight = targets*positive_weigh_factor  # All positive weights are equal to 10\n",
    "        criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item() * data1.size(0)  # Multiply by batch size\n",
    "\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Calculate average training loss for the epoch\n",
    "    epoch_train_loss /= len(train_loader.dataset)\n",
    "    adv_train_losses.append(epoch_train_loss)\n",
    "\n",
    "    \n",
    "    # Validation loop\n",
    "    advanced_model.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data1, data2, data3, targets, _ in val_loader:\n",
    "            data1 = data1.to(device)\n",
    "            data2 = data2.to(device)\n",
    "            data3 = data3.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = advanced_model(data1, data2, data3)\n",
    "            \n",
    "            pos_weight = targets * positive_weigh_factor\n",
    "            criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            epoch_val_loss += loss.item() * data1.size(0)\n",
    "    \n",
    "    # Calculate average validation loss\n",
    "    epoch_val_loss /= len(val_loader.dataset)\n",
    "    adv_val_losses.append(epoch_val_loss)\n",
    "    \n",
    "    # Save best model\n",
    "    if epoch_val_loss < best_val_loss:\n",
    "        best_val_loss = epoch_val_loss\n",
    "        torch.save(advanced_model.state_dict(), \"advanced_best_multimodal-model.pth\")\n",
    "        print(f\"New advanced best model saved with val loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "    print(f\"Train Loss: {epoch_train_loss:.4f} | Val Loss: {epoch_val_loss:.4f}\")\n",
    "\n",
    "# Save the advanced final model\n",
    "torch.save(advanced_model.state_dict(), \"final-multimodal-advanced-model.pth\")\n",
    "\n",
    "# Print training summary\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5999e22a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7097151124452508,\n",
       " 0.6934396136187879,\n",
       " 0.6932163436735475,\n",
       " 0.6931690513408285,\n",
       " 0.6931554204484858,\n",
       " 0.6931506949895686,\n",
       " 0.6931488288675616,\n",
       " 0.6931480212911286,\n",
       " 0.6931475269964791,\n",
       " 0.6931473797732935]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73bc6926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6937322382945517,\n",
       " 0.6933167394600052,\n",
       " 0.6932054519796215,\n",
       " 0.6931742205389753,\n",
       " 0.6931613724246724,\n",
       " 0.6931512896658097,\n",
       " 0.6931511135668608,\n",
       " 0.6931475337694103,\n",
       " 0.6931475179439038,\n",
       " 0.6931473148558573]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4c7e6e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved state dictionary\n",
    "advanced_model.load_state_dict(torch.load(\"/users/PAS2956/sandeep633/final-multimodal-advanced-model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c742001a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Sample #: 0\n",
      "Test Sample #: 50\n",
      "Test Sample #: 100\n",
      "Test Sample #: 150\n",
      "Test Sample #: 200\n",
      "Test Sample #: 250\n",
      "Test Sample #: 300\n",
      "Test Sample #: 350\n",
      "Test Sample #: 400\n",
      "Test Sample #: 450\n",
      "Test Sample #: 500\n",
      "Test Sample #: 550\n",
      "Test Sample #: 600\n",
      "Test Sample #: 650\n",
      "Test Sample #: 700\n",
      "Test Sample #: 750\n",
      "Test Sample #: 800\n",
      "Test Sample #: 850\n",
      "Test Sample #: 900\n",
      "Test Sample #: 950\n",
      "Test Sample #: 1000\n",
      "Test Sample #: 1050\n",
      "Test Sample #: 1100\n",
      "Test Sample #: 1150\n",
      "Test Sample #: 1200\n",
      "Test Sample #: 1250\n",
      "Test Sample #: 1300\n",
      "Test Sample #: 1350\n",
      "Test Sample #: 1400\n",
      "Test Sample #: 1450\n",
      "Test Sample #: 1500\n",
      "Test Sample #: 1550\n",
      "Test Sample #: 1600\n",
      "Test Sample #: 1650\n",
      "Test Sample #: 1700\n",
      "Test Sample #: 1750\n",
      "Test Sample #: 1800\n",
      "Test Sample #: 1850\n",
      "Test Sample #: 1900\n",
      "Test Sample #: 1950\n",
      "Test Sample #: 2000\n",
      "Test Sample #: 2050\n",
      "Test Sample #: 2100\n",
      "Test Sample #: 2150\n",
      "Test Sample #: 2200\n",
      "Test Sample #: 2250\n",
      "Test Sample #: 2300\n",
      "Test Sample #: 2350\n",
      "Test Sample #: 2400\n",
      "Test Sample #: 2450\n",
      "Test Sample #: 2500\n",
      "Test Sample #: 2550\n",
      "Test Sample #: 2600\n",
      "Test Sample #: 2650\n",
      "Test Sample #: 2700\n",
      "Test Sample #: 2750\n",
      "Test Sample #: 2800\n",
      "Test Sample #: 2850\n",
      "Test Sample #: 2900\n",
      "Test Sample #: 2950\n",
      "Test Sample #: 3000\n",
      "Test Sample #: 3050\n",
      "Test Sample #: 3100\n",
      "Test Sample #: 3150\n",
      "Test Sample #: 3200\n",
      "Test Sample #: 3250\n",
      "Test Sample #: 3300\n",
      "Test Sample #: 3350\n",
      "Test Sample #: 3400\n",
      "Test Sample #: 3450\n",
      "Test Sample #: 3500\n",
      "Test Sample #: 3550\n",
      "Test Sample #: 3600\n",
      "Test Sample #: 3650\n",
      "Test Sample #: 3700\n",
      "Test Sample #: 3750\n",
      "Test Sample #: 3800\n",
      "Test Sample #: 3850\n",
      "Test Sample #: 3900\n",
      "Test Sample #: 3950\n",
      "Test Sample #: 4000\n",
      "Test Sample #: 4050\n",
      "Test Sample #: 4100\n",
      "Test Sample #: 4150\n",
      "Test Sample #: 4200\n",
      "Test Sample #: 4250\n",
      "Test Sample #: 4300\n",
      "Test Sample #: 4350\n",
      "Test Sample #: 4400\n",
      "Test Sample #: 4450\n",
      "Test Sample #: 4500\n",
      "Test Sample #: 4550\n",
      "Test Sample #: 4600\n",
      "Test Sample #: 4650\n",
      "Test Sample #: 4700\n",
      "Test Sample #: 4750\n",
      "Test Sample #: 4800\n",
      "Test Sample #: 4850\n",
      "Test Sample #: 4900\n",
      "Test Sample #: 4950\n",
      "Test Sample #: 5000\n",
      "Test Sample #: 5050\n",
      "Test Sample #: 5100\n",
      "Test Sample #: 5150\n",
      "Test Sample #: 5200\n",
      "Test Sample #: 5250\n",
      "Test Sample #: 5300\n",
      "Test Sample #: 5350\n",
      "Test Sample #: 5400\n",
      "Test Sample #: 5450\n",
      "Test Sample #: 5500\n",
      "Test Sample #: 5550\n",
      "Test Sample #: 5600\n",
      "Test Sample #: 5650\n",
      "Test Sample #: 5700\n",
      "Test Sample #: 5750\n",
      "Test Sample #: 5800\n",
      "Test Sample #: 5850\n",
      "Test Sample #: 5900\n",
      "Test Sample #: 5950\n",
      "Test Sample #: 6000\n",
      "Test Sample #: 6050\n",
      "Test Sample #: 6100\n",
      "Test Sample #: 6150\n",
      "Test Sample #: 6200\n",
      "Test Sample #: 6250\n",
      "Test Sample #: 6300\n",
      "Test Sample #: 6350\n",
      "Test Sample #: 6400\n",
      "Test Sample #: 6450\n",
      "Test Sample #: 6500\n",
      "Test Sample #: 6550\n",
      "Test Sample #: 6600\n",
      "Test Sample #: 6650\n",
      "Test Sample #: 6700\n",
      "Test Sample #: 6750\n",
      "Test Sample #: 6800\n",
      "Test Sample #: 6850\n",
      "Test Sample #: 6900\n",
      "Test Sample #: 6950\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    all_predictions = []\n",
    "    surveys = []\n",
    "    adv_top_k_indices = None\n",
    "    for batch_idx, (data1, data2, data3, surveyID) in enumerate(test_loader):\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f\"Test Sample #: {batch_idx}\")\n",
    "        data1 = data1.to(device)\n",
    "        data2 = data2.to(device)\n",
    "        data3 = data3.to(device)\n",
    "\n",
    "        outputs = advanced_model(data1, data2, data3)\n",
    "        predictions = torch.sigmoid(outputs).cpu().numpy()\n",
    "\n",
    "        # Sellect top-25 values as predictions\n",
    "        top_25 = np.argsort(-predictions, axis=1)[:, :25] \n",
    "        if adv_top_k_indices is None:\n",
    "            adv_top_k_indices = top_25\n",
    "        else:\n",
    "            adv_top_k_indices = np.concatenate((adv_top_k_indices, top_25), axis=0)\n",
    "\n",
    "        surveys.extend([int(sid) for sid in surveyID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7f3138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data_concatenated = [' '.join(map(str, row)) for row in adv_top_k_indices]\n",
    "\n",
    "pd.DataFrame(\n",
    "    {'surveyId': surveys,\n",
    "     'predictions': adv_data_concatenated,\n",
    "    }).to_csv(\"Final_Multimodal_Advanced.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63f3e804",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_predicted = pd.DataFrame(\n",
    "    {'surveyId': surveys,\n",
    "     'predictions': adv_data_concatenated,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12874a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Accuracy (is true species in top-25?): 0.0113\n",
      "Micro F1-score: 0.0087\n",
      "Macro F1-score: 0.0005\n"
     ]
    }
   ],
   "source": [
    "evaluate_predictions(adv_predicted, test_metadata, top_k=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c0ec49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd18fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
