{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7dd422c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import rasterio  \n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "24b7fd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    # Set seed\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e1401a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = CUDA\n"
     ]
    }
   ],
   "source": [
    "# Check if cuda is available\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"DEVICE = CUDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f5135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_patch_path(data_path, survey_id):\n",
    "    \"\"\"Construct the patch file path based on plot_id as './CD/AB/XXXXABCD.tiff'\"\"\"\n",
    "    path = data_path\n",
    "    for d in (str(survey_id)[-2:], str(survey_id)[-4:-2]):\n",
    "        path = os.path.join(path, d)\n",
    "\n",
    "    path = os.path.join(path, f\"{survey_id}.tiff\")\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a8839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_normalize(band, low=2, high=98):\n",
    "    sorted_band = np.sort(band.flatten())\n",
    "    quantiles = np.percentile(sorted_band, np.linspace(low, high, len(sorted_band)))\n",
    "    normalized_band = np.interp(band.flatten(), sorted_band, quantiles).reshape(band.shape)\n",
    "    \n",
    "    min_val, max_val = np.min(normalized_band), np.max(normalized_band)\n",
    "    \n",
    "    # Prevent division by zero if min_val == max_val\n",
    "    if max_val == min_val:\n",
    "        return np.zeros_like(normalized_band, dtype=np.float32)  # Return an array of zeros\n",
    "\n",
    "    # Perform normalization (min-max scaling)\n",
    "    return ((normalized_band - min_val) / (max_val - min_val)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5521bf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentinel_TrainDataset(Dataset):\n",
    "    def __init__(self, data_dir, metadata, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data_dir = data_dir\n",
    "        self.metadata = metadata\n",
    "        self.metadata = self.metadata.dropna(subset=\"speciesId\").reset_index(drop=True)\n",
    "        self.metadata['speciesId'] = self.metadata['speciesId'].astype(int)\n",
    "        self.label_dict = self.metadata.groupby('surveyId')['speciesId'].apply(list).to_dict()\n",
    "        \n",
    "        self.metadata = self.metadata.drop_duplicates(subset=\"surveyId\").reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        survey_id = self.metadata.surveyId[idx]\n",
    "        species_ids = self.label_dict.get(survey_id, [])  # Get list of species IDs for the survey ID\n",
    "        label = torch.zeros(num_classes)  # Initialize label tensor\n",
    "        for species_id in species_ids:\n",
    "            label_id = species_id\n",
    "            label[label_id] = 1  # Set the corresponding class index to 1 for each species\n",
    "        \n",
    "        # Read TIFF files (multispectral bands)\n",
    "        tiff_path = construct_patch_path(self.data_dir, survey_id)\n",
    "        with rasterio.open(tiff_path) as dataset:\n",
    "            image = dataset.read(out_dtype=np.float32)  # Read all bands\n",
    "            image = np.array([quantile_normalize(band) for band in image])  # Apply quantile normalization\n",
    "\n",
    "        image = np.transpose(image, (1, 2, 0))  # Convert to HWC format\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return image, label, survey_id\n",
    "    \n",
    "class Sentinel_TestDataset(Sentinel_TrainDataset):\n",
    "    def __init__(self, data_dir, metadata, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data_dir = data_dir\n",
    "        self.metadata = metadata\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        survey_id = self.metadata.surveyId[idx]\n",
    "        \n",
    "        # Read TIFF files (multispectral bands)\n",
    "        tiff_path = construct_patch_path(self.data_dir, survey_id)\n",
    "        with rasterio.open(tiff_path) as dataset:\n",
    "            image = dataset.read(out_dtype=np.float32)  # Read all bands\n",
    "            image = np.array([quantile_normalize(band) for band in image])  # Apply quantile normalization\n",
    "\n",
    "        image = np.transpose(image, (1, 2, 0))  # Convert to HWC format\n",
    "        \n",
    "        image = self.transform(image)\n",
    "        return image, survey_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9aa13db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Climate_TrainDataset(Dataset):\n",
    "    def __init__(self, data_dir, metadata, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        self.data_dir = data_dir\n",
    "        self.metadata = metadata\n",
    "        self.metadata = self.metadata.dropna(subset=\"speciesId\").reset_index(drop=True)\n",
    "        self.metadata['speciesId'] = self.metadata['speciesId'].astype(int)\n",
    "        self.label_dict = self.metadata.groupby('surveyId')['speciesId'].apply(list).to_dict()\n",
    "        \n",
    "        self.metadata = self.metadata.drop_duplicates(subset=\"surveyId\").reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        survey_id = self.metadata.surveyId[idx]\n",
    "        sample = torch.load(os.path.join(self.data_dir, f\"GLC25-PA-{self.subset}-bioclimatic_monthly_{survey_id}_cube.pt\"), weights_only=True)\n",
    "        species_ids = self.label_dict.get(survey_id, [])  # Get list of species IDs for the survey ID\n",
    "        label = torch.zeros(num_classes)  \n",
    "        for species_id in species_ids:\n",
    "            label_id = species_id\n",
    "            label[label_id] = 1  # Set the corresponding class index to 1 for each species\n",
    "\n",
    "        # Ensure the sample is in the correct format for the transform\n",
    "        if isinstance(sample, torch.Tensor):\n",
    "            sample = sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            sample = sample.numpy()  \n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, label, survey_id\n",
    "    \n",
    "class Climate_TestDataset(Climate_TrainDataset):\n",
    "    def __init__(self, data_dir, metadata, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        self.data_dir = data_dir\n",
    "        self.metadata = metadata\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        survey_id = self.metadata.surveyId[idx]\n",
    "        sample = torch.load(os.path.join(self.data_dir, f\"GLC25-PA-train-bioclimatic_monthly_{survey_id}_cube.pt\"), weights_only=True)\n",
    "        \n",
    "        if isinstance(sample, torch.Tensor):\n",
    "            sample = sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            sample = sample.numpy()\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, survey_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fb37b98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Landsat_TrainDataset(Dataset):\n",
    "    def __init__(self, data_dir, metadata, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        self.data_dir = data_dir\n",
    "        self.metadata = metadata\n",
    "        self.metadata = self.metadata.dropna(subset=\"speciesId\").reset_index(drop=True)\n",
    "        self.metadata['speciesId'] = self.metadata['speciesId'].astype(int)\n",
    "        self.label_dict = self.metadata.groupby('surveyId')['speciesId'].apply(list).to_dict()\n",
    "        self.metadata = self.metadata.drop_duplicates(subset=\"surveyId\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        survey_id = self.metadata.surveyId[idx]\n",
    "        sample = torch.nan_to_num(torch.load(os.path.join(self.data_dir, f\"GLC25-PA-{self.subset}-landsat-time-series_{survey_id}_cube.pt\"), weights_only=True))\n",
    "        species_ids = self.label_dict.get(survey_id, [])  # Get list of species IDs for the survey ID\n",
    "        label = torch.zeros(num_classes)  # Initialize label tensor\n",
    "        for species_id in species_ids:\n",
    "            label_id = species_id\n",
    "            label[label_id] = 1  # Set the corresponding class index to 1 for each species\n",
    "\n",
    "        if isinstance(sample, torch.Tensor):\n",
    "            sample = sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            sample = sample.numpy()  # Convert tensor to numpy array\n",
    "            #print(sample.shape)\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, label, survey_id\n",
    "    \n",
    "class Landsat_TestDataset(Dataset):\n",
    "    def __init__(self, data_dir, metadata, subset, transform=None):\n",
    "        super().__init__()\n",
    "        self.data_dir  = data_dir\n",
    "        self.metadata  = metadata.reset_index(drop=True)\n",
    "        self.subset    = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        survey_id = self.metadata.surveyId[idx]\n",
    "\n",
    "        fname = f\"GLC25-PA-{self.subset}-landsat-time-series_{survey_id}_cube.pt\"\n",
    "        path  = os.path.join(self.data_dir, fname)\n",
    "        sample = torch.nan_to_num(torch.load(path, weights_only=True))\n",
    "\n",
    "        if isinstance(sample, torch.Tensor):\n",
    "            sample = sample.permute(1,2,0).numpy()\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, survey_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a3eded",
   "metadata": {},
   "source": [
    "# Advanced Model-1 (Sentinel, Landsat, and BioClimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ca64b9",
   "metadata": {},
   "source": [
    "### Data and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "041da54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "CLIMATE_DIR = \"/fs/scratch/PAS2985/group_23/BioclimTimeSeries/cubes/PA-train\"\n",
    "LANDSAT_DIR = \"/fs/scratch/PAS2985/group_23/SateliteTimeSeries-Landsat/cubes/PA-train/\"\n",
    "SENTINEL_DIR = \"/fs/scratch/PAS2985/group_23/SatelitePatches/PA-train\"\n",
    "\n",
    "CLIMATE_TRAIN_DIR = \"/fs/scratch/PAS2985/group_23/BioclimTimeSeries/cubes/PA-train\"\n",
    "CLIMATE_TEST_DIR = \"/fs/scratch/PAS2985/group_23/BioclimTimeSeries/cubes/PA-train\"\n",
    "CLIMATE_META_TRAIN = \"/fs/scratch/PAS2985/group_23/training_data.csv\"\n",
    "CLIMATE_META_TEST = \"/fs/scratch/PAS2985/group_23/test_data.csv\"\n",
    "\n",
    "LANDSAT_TRAIN_DIR = \"/fs/scratch/PAS2985/group_23/SateliteTimeSeries-Landsat/cubes/PA-train/\"\n",
    "LANDSAT_TEST_DIR = \"/fs/scratch/PAS2985/group_23/SateliteTimeSeries-Landsat/cubes/PA-train/\"\n",
    "LANDSAT_META_TRAIN = \"/fs/scratch/PAS2985/group_23/training_data.csv\"\n",
    "LANDSAT_META_TEST = \"/fs/scratch/PAS2985/group_23/test_data.csv\"\n",
    "\n",
    "SENTINEL_TRAIN_DIR = \"/fs/scratch/PAS2985/group_23/SatelitePatches/PA-train\"\n",
    "SENTINEL_TEST_DIR = \"/fs/scratch/PAS2985/group_23/SatelitePatches/PA-train\"\n",
    "SENTINEL_META_TRAIN = \"/fs/scratch/PAS2985/group_23/training_data.csv\"\n",
    "SENTINEL_META_TEST = \"/fs/scratch/PAS2985/group_23/test_data.csv\"\n",
    "\n",
    "NUM_CLASSES = 11255\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-4\n",
    "EPOCHS = 20\n",
    "POS_WEIGHT = 1.0  # Positive weight factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d0c98f",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "54b1c7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = NUM_CLASSES\n",
    "\n",
    "climate_transform = transforms.Compose([transforms.ToTensor()])\n",
    "landsat_transform = transforms.Compose([transforms.ToTensor()])\n",
    "sentinel_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,)*4, std=(0.5,)*4)\n",
    "])\n",
    "\n",
    "# load metadata\n",
    "meta_train = pd.read_csv(\"/fs/scratch/PAS2985/group_23/training_data.csv\")\n",
    "meta_test = pd.read_csv(\"/fs/scratch/PAS2985/group_23/test_data.csv\")\n",
    "\n",
    "# Initiate datasets\n",
    "cl_train_ds = Climate_TrainDataset(CLIMATE_TRAIN_DIR, cl_meta_tr, subset=\"train\", transform=climate_transform)\n",
    "cl_test_ds = Climate_TestDataset(CLIMATE_TRAIN_DIR, cl_meta_te, subset=\"train\", transform=climate_transform)\n",
    "\n",
    "ls_train_ds = Landsat_TrainDataset(LANDSAT_TRAIN_DIR, ls_meta_tr, subset=\"train\", transform=landsat_transform)\n",
    "ls_test_ds = Landsat_TestDataset(LANDSAT_TRAIN_DIR, ls_meta_te, subset=\"train\", transform=landsat_transform)\n",
    "\n",
    "se_train_ds = Sentinel_TrainDataset(SENTINEL_TRAIN_DIR, se_meta_tr, transform=sentinel_transform)\n",
    "se_test_ds = Sentinel_TestDataset(SENTINEL_TRAIN_DIR, se_meta_te, transform=sentinel_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dc7fc54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modalities train sizes: 62290 62290 62290\n",
      "Modalities  test sizes: 223031 223031 223031\n"
     ]
    }
   ],
   "source": [
    "cl_train_ds = Climate_TrainDataset(CLIMATE_DIR, meta_train, subset=\"train\", transform=climate_transform)\n",
    "cl_test_ds = Climate_TestDataset(CLIMATE_DIR, meta_test, subset=\"train\", transform=climate_transform)\n",
    "\n",
    "ls_train_ds = Landsat_TrainDataset(LANDSAT_DIR, meta_train, subset=\"train\", transform=landsat_transform)\n",
    "ls_test_ds = Landsat_TestDataset(LANDSAT_DIR, meta_test, subset=\"train\", transform=landsat_transform)\n",
    "\n",
    "se_train_ds = Sentinel_TrainDataset(SENTINEL_DIR, meta_train, transform=sentinel_transform)\n",
    "se_test_ds = Sentinel_TestDataset (SENTINEL_DIR, meta_test, transform=sentinel_transform)\n",
    "\n",
    "print(\"Modalities train sizes:\", len(cl_train_ds), len(ls_train_ds), len(se_train_ds))\n",
    "print(\"Modalities  test sizes:\", len(cl_test_ds), len(ls_test_ds), len(se_test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2a875e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CombinedDataset: returns ((cl_img, ls_img, se_img), label, survey_id)\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CombinedTrainDataset(Dataset):\n",
    "    def __init__(self, ds_cl, ds_ls, ds_se):\n",
    "        assert len(ds_cl)==len(ds_ls)==len(ds_se), \"modalities must align\"\n",
    "        self.cl, self.ls, self.se = ds_cl, ds_ls, ds_se\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cl)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # each returns (img, label, survey_id)\n",
    "        x_cl, y, id_cl = self.cl[idx]\n",
    "        x_ls, _, _     = self.ls[idx]\n",
    "        x_se, _, _     = self.se[idx]\n",
    "        return (x_cl, x_ls, x_se), y, id_cl\n",
    "\n",
    "class CombinedTestDataset(Dataset):\n",
    "    def __init__(self, ds_cl, ds_ls, ds_se):\n",
    "        assert len(ds_cl)==len(ds_ls)==len(ds_se), \"modalities must align\"\n",
    "        self.cl, self.ls, self.se = ds_cl, ds_ls, ds_se\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cl)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # each returns (img, survey_id)\n",
    "        x_cl, id_cl = self.cl[idx]\n",
    "        x_ls, id_ls = self.ls[idx]\n",
    "        x_se, id_se = self.se[idx]\n",
    "        # sanity check that IDs match\n",
    "        assert id_cl == id_ls == id_se\n",
    "        return (x_cl, x_ls, x_se), id_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b85043b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches per epoch: 1947\n"
     ]
    }
   ],
   "source": [
    "train_ds = CombinedTrainDataset(cl_train_ds, ls_train_ds, se_train_ds)\n",
    "test_ds = CombinedTestDataset(cl_test_ds, ls_test_ds, se_test_ds)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "print(\"Batches per epoch:\", len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbd49b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches per epoch: 1947\n",
      "CombinedModel(\n",
      "  (climate): ResNet(\n",
      "    (conv1): Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): Identity()\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Identity()\n",
      "  )\n",
      "  (landsat): ResNet(\n",
      "    (conv1): Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): Identity()\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Identity()\n",
      "  )\n",
      "  (sentinel): ResNet(\n",
      "    (conv1): Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): Identity()\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Identity()\n",
      "  )\n",
      "  (classifier): Linear(in_features=1536, out_features=11255, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.climate = models.resnet18(weights=None)\n",
    "        self.climate.conv1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.climate.maxpool = nn.Identity()\n",
    "        self.climate.fc = nn.Identity()\n",
    "\n",
    "        self.landsat = models.resnet18(weights=None)\n",
    "        self.landsat.conv1 = nn.Conv2d(6, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.landsat.maxpool = nn.Identity()\n",
    "        self.landsat.fc = nn.Identity()\n",
    "\n",
    "        self.sentinel = models.resnet18(weights=None)\n",
    "        self.sentinel.conv1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.sentinel.maxpool = nn.Identity()\n",
    "        self.sentinel.fc = nn.Identity()\n",
    "\n",
    "        # Final classifier\n",
    "        self.classifier = nn.Linear(512*3, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x_cl, x_ls, x_se = inputs\n",
    "        f_cl = self.climate(x_cl)\n",
    "        f_ls = self.landsat(x_ls)\n",
    "        f_se = self.sentinel(x_se)\n",
    "        # concatenate and classify\n",
    "        cat = torch.cat([f_cl, f_ls, f_se], dim=1)\n",
    "        return self.classifier(cat)\n",
    "\n",
    "model = CombinedModel(NUM_CLASSES).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff343609",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PAS2985/patwardhan24/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 — Loss: 0.0093\n",
      "Epoch 2/20 — Loss: 0.0047\n",
      "Epoch 3/20 — Loss: 0.0044\n",
      "Epoch 4/20 — Loss: 0.0042\n",
      "Epoch 5/20 — Loss: 0.0040\n",
      "Epoch 6/20 — Loss: 0.0038\n",
      "Epoch 7/20 — Loss: 0.0036\n",
      "Epoch 8/20 — Loss: 0.0034\n",
      "Epoch 9/20 — Loss: 0.0033\n",
      "Epoch 10/20 — Loss: 0.0031\n",
      "Epoch 11/20 — Loss: 0.0029\n",
      "Epoch 12/20 — Loss: 0.0028\n",
      "Epoch 13/20 — Loss: 0.0027\n",
      "Epoch 14/20 — Loss: 0.0026\n",
      "Epoch 15/20 — Loss: 0.0025\n",
      "Epoch 16/20 — Loss: 0.0024\n",
      "Epoch 17/20 — Loss: 0.0023\n",
      "Epoch 18/20 — Loss: 0.0023\n",
      "Epoch 19/20 — Loss: 0.0022\n",
      "Epoch 20/20 — Loss: 0.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6970 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/users/PAS2985/patwardhan24/.local/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/users/PAS2985/patwardhan24/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/users/PAS2985/patwardhan24/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/slurmtmp.587255/ipykernel_3629020/3478093453.py\", line 11, in __getitem__\n    x_cl, y, id_cl = self.cl[idx]\n  File \"/tmp/slurmtmp.587255/ipykernel_3629020/2366138481.py\", line 46, in __getitem__\n    sample = torch.load(os.path.join(self.data_dir, f\"GLC25-PA-{self.subset}-bioclimatic_monthly_{survey_id}_cube.pt\"), weights_only=True)\n  File \"/users/PAS2985/patwardhan24/.local/lib/python3.9/site-packages/torch/serialization.py\", line 1425, in load\n    with _open_file_like(f, \"rb\") as opened_file:\n  File \"/users/PAS2985/patwardhan24/.local/lib/python3.9/site-packages/torch/serialization.py\", line 751, in _open_file_like\n    return _open_file(name_or_buffer, mode)\n  File \"/users/PAS2985/patwardhan24/.local/lib/python3.9/site-packages/torch/serialization.py\", line 732, in __init__\n    super().__init__(open(name, mode))\nFileNotFoundError: [Errno 2] No such file or directory: '/fs/scratch/PAS2985/group_23/BioclimTimeSeries/cubes/PA-train/GLC25-PA-test-bioclimatic_monthly_489_cube.pt'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m all_preds, all_ids \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (x_cl, x_ls, x_se), _, ids \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(test_loader, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_loader)):\n\u001b[1;32m     33\u001b[0m         x_cl, x_ls, x_se \u001b[38;5;241m=\u001b[39m x_cl\u001b[38;5;241m.\u001b[39mto(device), x_ls\u001b[38;5;241m.\u001b[39mto(device), x_se\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     34\u001b[0m         logits \u001b[38;5;241m=\u001b[39m model((x_cl, x_ls, x_se))\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1480\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1505\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1505\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_utils.py:733\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 733\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/users/PAS2985/patwardhan24/.local/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/users/PAS2985/patwardhan24/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/users/PAS2985/patwardhan24/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/slurmtmp.587255/ipykernel_3629020/3478093453.py\", line 11, in __getitem__\n    x_cl, y, id_cl = self.cl[idx]\n  File \"/tmp/slurmtmp.587255/ipykernel_3629020/2366138481.py\", line 46, in __getitem__\n    sample = torch.load(os.path.join(self.data_dir, f\"GLC25-PA-{self.subset}-bioclimatic_monthly_{survey_id}_cube.pt\"), weights_only=True)\n  File \"/users/PAS2985/patwardhan24/.local/lib/python3.9/site-packages/torch/serialization.py\", line 1425, in load\n    with _open_file_like(f, \"rb\") as opened_file:\n  File \"/users/PAS2985/patwardhan24/.local/lib/python3.9/site-packages/torch/serialization.py\", line 751, in _open_file_like\n    return _open_file(name_or_buffer, mode)\n  File \"/users/PAS2985/patwardhan24/.local/lib/python3.9/site-packages/torch/serialization.py\", line 732, in __init__\n    super().__init__(open(name, mode))\nFileNotFoundError: [Errno 2] No such file or directory: '/fs/scratch/PAS2985/group_23/BioclimTimeSeries/cubes/PA-train/GLC25-PA-test-bioclimatic_monthly_489_cube.pt'\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS, verbose=True)\n",
    "\n",
    "def criterion_fn(outputs, targets):\n",
    "    pos_weight = targets * POS_WEIGHT\n",
    "    return nn.BCEWithLogitsLoss(pos_weight=pos_weight)(outputs, targets)\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for (x_cl, x_ls, x_se), labels, _ in train_loader:\n",
    "        x_cl, x_ls, x_se, labels = x_cl.to(device), x_ls.to(device), x_se.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model((x_cl, x_ls, x_se))\n",
    "        loss = criterion_fn(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x_cl.size(0)\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} — Loss: {avg_loss:.4f}\")\n",
    "\n",
    "model.eval()\n",
    "all_preds, all_ids = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77874c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"combined_model_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c6489d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6970/6970 [39:34<00:00,  2.94it/s]\n"
     ]
    }
   ],
   "source": [
    "all_preds, all_ids = [], []\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (x_cl, x_ls, x_se), survey_id in tqdm.tqdm(test_loader):\n",
    "        x_cl, x_ls, x_se = x_cl.to(device), x_ls.to(device), x_se.to(device)\n",
    "        logits = model((x_cl, x_ls, x_se))\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "        top25 = np.argsort(-probs, axis=1)[:, :25]\n",
    "        all_preds.append(top25)\n",
    "        all_ids.extend(survey_id.numpy())\n",
    "\n",
    "all_preds = np.vstack(all_preds)\n",
    "pred_str = [\" \".join(map(str, row)) for row in all_preds]\n",
    "submission = pd.DataFrame({'surveyId': all_ids, 'predictions': pred_str})\n",
    "submission.to_csv(\"combined_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbcf13d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
